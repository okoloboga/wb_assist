import json
from typing import Any, Dict, List, Optional

from gpt_integration.gpt_client import GPTClient
from gpt_integration.analysis.template_loader import (
    get_system_prompt,
    get_tasks,
    get_output_json_schema,
    get_output_tg_guide,
)
from utils.formatters import escape_markdown_v2, split_telegram_message


def _log_data_summary(data: Dict[str, Any]) -> None:
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –¥–∞–Ω–Ω—ã—Ö, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö –≤ GPT."""
    import logging
    logger = logging.getLogger(__name__)
    
    logger.debug(f"  data keys: {list(data.keys())}")
    
    # Meta
    meta = data.get("meta") or {}
    logger.debug(f"  meta: telegram_id={meta.get('telegram_id')}, period={meta.get('period')}, days_window={meta.get('days_window')}")
    
    # Yesterday
    yesterday = data.get("yesterday") or {}
    if yesterday:
        logger.debug(f"  yesterday: date={yesterday.get('date')}, orders={yesterday.get('orders')}, "
                    f"orders_amount={yesterday.get('orders_amount')}, top_products_count={len(yesterday.get('top_products', []))}")
    else:
        logger.debug(f"  yesterday: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
    
    # Daily trends
    daily_trends = data.get("daily_trends") or {}
    if daily_trends:
        dt_meta = daily_trends.get("meta") or {}
        dt_agg = daily_trends.get("aggregates") or {}
        ts = daily_trends.get("time_series") or []
        logger.debug(f"  daily_trends: days_window={dt_meta.get('days_window')}, time_series_len={len(ts)}")
        logger.debug(f"    aggregates: buyout_rate={dt_agg.get('conversion', {}).get('buyout_rate_percent')}%, "
                    f"return_rate={dt_agg.get('conversion', {}).get('return_rate_percent')}%")
        if ts:
            logger.debug(f"    time_series: –ø–µ—Ä–≤–∞—è –¥–∞—Ç–∞={ts[0].get('date')}, –ø–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞={ts[-1].get('date')}")
    else:
        logger.debug(f"  daily_trends: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
    
    # Other sources
    for key in ["stocks_critical", "reviews_summary", "orders_recent", "sales"]:
        val = data.get(key)
        if val:
            if isinstance(val, dict):
                logger.debug(f"  {key}: –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç (keys: {list(val.keys())[:5]})")
            elif isinstance(val, list):
                logger.debug(f"  {key}: –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç (len={len(val)})")
            else:
                logger.debug(f"  {key}: –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç (type={type(val).__name__})")
        else:
            logger.debug(f"  {key}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
    
    # –†–∞–∑–º–µ—Ä JSON
    try:
        data_json = json.dumps(data, ensure_ascii=False)
        logger.debug(f"  JSON size: {len(data_json)} —Å–∏–º–≤–æ–ª–æ–≤ (~{len(data_json.encode('utf-8'))} bytes)")
    except Exception as e:
        logger.debug(f"  JSON size: –Ω–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å ({e})")


def compose_messages(data: Dict[str, Any], template_path: Optional[str] = None) -> List[Dict[str, str]]:
    """
    Compose LLM messages using SYSTEM + DATA + TASKS (+ optional schema/guides).
    Returns OpenAI chat messages list.
    """
    import logging
    logger = logging.getLogger(__name__)
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –¥–∞–Ω–Ω—ã—Ö
    _log_data_summary(data)
    
    system = get_system_prompt(template_path) or (
        "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ e‚Äëcommerce. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –Ω–∞ —Ä—É—Å—Å–∫–æ–º."
    )
    tasks = get_tasks(template_path)
    schema = get_output_json_schema(template_path)
    tg_guide = get_output_tg_guide(template_path)

    data_json = json.dumps(data, ensure_ascii=False, indent=2)
    sections: List[str] = [f"## DATA\n{data_json}"]
    if tasks:
        sections.append(f"## TASKS\n{tasks}")
    if schema:
        sections.append(f"## OUTPUT_JSON_SCHEMA\n{schema}")
    if tg_guide:
        sections.append(f"## OUTPUT_TG_GUIDE\n{tg_guide}")
    sections.append(
        "–í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ OUTPUT_JSON –∏ OUTPUT_TG, \n"
        "—Å–æ–±–ª—é–¥–∞—è –æ–ø–∏—Å–∞–Ω–Ω—ã–µ —Å—Ö–µ–º—ã –∏ –ø—Ä–∞–≤–∏–ª–∞.\n\n"
        "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ JSON –ë–ï–ó markdown –±–ª–æ–∫–æ–≤ (–±–µ–∑ ```json –∏ ```). "
        "–ù–∞—á–Ω–∏ –æ—Ç–≤–µ—Ç —Å—Ä–∞–∑—É —Å –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π —Ñ–∏–≥—É—Ä–Ω–æ–π —Å–∫–æ–±–∫–∏ { –∏ –∑–∞–∫–æ–Ω—á–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π }."
    )
    user_content = "\n\n".join(sections)
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞
    total_size = len(system) + len(user_content)

    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user_content},
    ]


def _safe_json_extract(text: str) -> Optional[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –æ–±—ä–µ–∫—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    –¢–µ–ø–µ—Ä—å –æ–∂–∏–¥–∞–µ—Ç —á–∏—Å—Ç—ã–π JSON –±–µ–∑ markdown –±–ª–æ–∫–æ–≤, –Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç fallback –¥–ª—è markdown.
    """
    import re
    import logging
    
    logger = logging.getLogger(__name__)
    
    if not text or not isinstance(text, str):
        return None
    
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    cleaned = text.strip()
    
    def _sanitize_numeric_underscores(s: str) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ —á–∏—Å–µ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, 40_000.0 -> 40000.0), –Ω–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—è —Å—Ç—Ä–æ–∫–∏.
        """
        # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤–Ω–µ —Å—Ç—Ä–æ–∫ JSON (–≥—Ä—É–±–∞—è, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è):
        out = []
        in_string_local = False
        escape = False
        for ch in s:
            if escape:
                out.append(ch)
                escape = False
                continue
            if ch == '\\\\':
                out.append(ch)
                escape = True
                continue
            if ch == '\"':
                in_string_local = not in_string_local
                out.append(ch)
                continue
            if not in_string_local and ch == '_':
                # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è –≤–Ω–µ —Å—Ç—Ä–æ–∫
                continue
            out.append(ch)
        return ''.join(out)
    
    def _sanitize_numeric_currency(s: str) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç —Å–∏–º–≤–æ–ª—ã –≤–∞–ª—é—Ç—ã –ø–æ—Å–ª–µ —á–∏—Å–µ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, 112500‚ÇΩ -> 112500, 1071.43‚ÇΩ -> 1071.43, -13000‚ÇΩ -> -13000), –Ω–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—è —Å—Ç—Ä–æ–∫–∏.
        """
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —á–∏—Å–µ–ª —Å —Å–∏–º–≤–æ–ª–æ–º –≤–∞–ª—é—Ç—ã –ø–æ—Å–ª–µ –Ω–∏—Ö
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —á–∏—Å–ª–∞ –∏ —á–∏—Å–ª–∞ —Å –ø–ª–∞–≤–∞—é—â–µ–π —Ç–æ—á–∫–æ–π
        currency_chars = r'‚ÇΩ\$\‚Ç¨¬£¬•'
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –º–∏–Ω—É—Å + —á–∏—Å–ª–æ (—Å —Ç–æ—á–∫–æ–π –∏–ª–∏ –±–µ–∑) + –≤–∞–ª—é—Ç–∞ + –∑–∞–ø—è—Ç–∞—è/–∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞/–∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Ñ–∏–≥—É—Ä–Ω–∞—è —Å–∫–æ–±–∫–∞/–∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏
        # –ü—Ä–∏–º–µ—Ä—ã: "112500‚ÇΩ,", "-13000‚ÇΩ,", "1071.43‚ÇΩ,", "112500‚ÇΩ}"
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º } –≤ f-string, —É–¥–≤–∞–∏–≤–∞—è –µ–≥–æ
        pattern = rf'(-?\d+\.?\d*)([{currency_chars}]+)(\s*[,}}\]]|\s*$)'
        
        def replace_currency(match):
            number = match.group(1)
            after = match.group(3)
            return number + after
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–µ–Ω—É
        result = re.sub(pattern, replace_currency, s)
        return result
    
    def extract_json_from_text(text_to_parse: str, description: str) -> Optional[Dict[str, Any]]:
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        start = text_to_parse.find("{")
        if start == -1:
            return None
        
        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–±–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏
        brace_count = 0
        end_pos = -1
        in_string = False
        escape_next = False
        
        for i in range(start, len(text_to_parse)):
            char = text_to_parse[i]
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            if escape_next:
                escape_next = False
                continue
            if char == '\\':
                escape_next = True
                continue
            if char == '"':
                in_string = not in_string
                continue
            
            # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–±–∫–∏ —Ç–æ–ª—å–∫–æ –≤–Ω–µ —Å—Ç—Ä–æ–∫
            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_pos = i
                        break
        
        if end_pos <= start:
            return None
        
        snippet = text_to_parse[start : end_pos + 1]
        
        # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        fixing_strategies = [
            lambda s: s,  # –û—Ä–∏–≥–∏–Ω–∞–ª
            lambda s: _sanitize_numeric_underscores(s),  # –£–¥–∞–ª—è–µ–º –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è –≤ —á–∏—Å–ª–∞—Ö
            lambda s: _sanitize_numeric_currency(s),  # –£–¥–∞–ª—è–µ–º —Å–∏–º–≤–æ–ª—ã –≤–∞–ª—é—Ç—ã –∏–∑ —á–∏—Å–µ–ª
            lambda s: _sanitize_numeric_underscores(_sanitize_numeric_currency(s)),  # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è: –≤–∞–ª—é—Ç–∞ + –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è
            lambda s: re.sub(r',(\s*[}\]])', r'\\1', s),  # –£–¥–∞–ª—è–µ–º trailing commas
            lambda s: re.sub(r',(\s*[}\]])', r'\\1', s).replace('\\\\n', '\\n'),  # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º newlines
        ]
        
        for strategy_idx, strategy in enumerate(fixing_strategies):
            try:
                fixed = strategy(snippet)
                parsed = json.loads(fixed)
                if isinstance(parsed, dict):
                    if strategy_idx == 0:
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω JSON –∏–∑ {description} ({len(snippet)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    else:
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω JSON –∏–∑ {description} –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π #{strategy_idx}")
                    return parsed
            except json.JSONDecodeError as e:
                if strategy_idx == 0:
                    logger.debug(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ {description} –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {e.pos}: {e.msg}")
                    error_pos = e.pos if hasattr(e, 'pos') else 0
                    logger.debug(f"–ü—Ä–æ–±–ª–µ–º–Ω–∞—è –æ–±–ª–∞—Å—Ç—å: {snippet[max(0, error_pos-50):error_pos+50]}")
                continue
            except Exception as e:
                logger.debug(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è #{strategy_idx} –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {description}: {e}")
                continue
        
        return None
    
    # –®–∞–≥ 1: –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —á–∏—Å—Ç—ã–π JSON (–Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å—Ä–∞–∑—É —Å {)
    # –≠—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–ª—É—á–∞–π - —á–∏—Å—Ç—ã–π JSON –±–µ–∑ markdown
    result = extract_json_from_text(cleaned, "—á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
    if result:
        return result
    
    # –®–∞–≥ 2: Fallback - –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–∏—Å—Ç—ã–π JSON, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ markdown –±–ª–æ–∫–∞—Ö
    # (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ GPT –≤—Å–µ –µ—â–µ –≤–µ—Ä–Ω—É–ª markdown)
    logger.debug("–ß–∏—Å—Ç—ã–π JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º markdown –±–ª–æ–∫–∏ –∫–∞–∫ fallback...")
    
    code_block_patterns = [
        r'```json\s*(.*?)\s*```',  # ```json ... ```
        r'```JSON\s*(.*?)\s*```',  # ```JSON ... ```
        r'```\s*(.*?)\s*```',      # ``` ... ``` (fallback)
    ]
    
    for pattern in code_block_patterns:
        code_blocks = re.findall(pattern, cleaned, re.DOTALL | re.IGNORECASE)
        
        for block_content in code_blocks:
            if not block_content.strip():
                continue
            
            result = extract_json_from_text(block_content.strip(), "markdown –±–ª–æ–∫–∞")
            if result:
                return result
    
    # –®–∞–≥ 3: –£–¥–∞–ª—è–µ–º markdown –±–ª–æ–∫–∏ –∏ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –≤ –æ—Å—Ç–∞–≤—à–µ–º—Å—è —Ç–µ–∫—Å—Ç–µ
    cleaned_no_markdown = re.sub(r'```(?:json|JSON)?\s*.*?\s*```', '', cleaned, flags=re.DOTALL | re.IGNORECASE)
    if cleaned_no_markdown != cleaned:
        result = extract_json_from_text(cleaned_no_markdown.strip(), "—Ç–µ–∫—Å—Ç–∞ –±–µ–∑ markdown")
        if result:
            return result
    
    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π")
    # –õ–æ–≥–∏—Ä—É–µ–º –æ–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –æ—Ç–ª–∞–¥–∫–µ
    if len(text) > 500:
        logger.error(f"–û–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {text[:500]}")
        logger.error(f"–û–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {text[-500:]}")
    else:
        logger.error(f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}")
    return None


def _prepare_telegram_from_text(text: str) -> Dict[str, Any]:
    # Remove MarkdownV2 escaping if present (bot sends without parse_mode)
    # This handles cases where LLM accidentally escaped text
    unescaped = _unescape_markdown_v2(text)
    chunks = split_telegram_message(unescaped)
    character_count = sum(len(c) for c in chunks)
    return {"chunks": chunks, "character_count": character_count}


def _unescape_markdown_v2(text: str) -> str:
    """Remove MarkdownV2 escaping characters."""
    if not text:
        return ""
    # Remove backslashes before special characters
    import re
    # Pattern matches backslash followed by special MarkdownV2 characters
    specials = r"_\[\]()~`>#+-=|{}.!*\\"
    # Replace \X with X (where X is a special character)
    return re.sub(r'\\([' + re.escape(specials) + r'])', r'\1', text)


def _normalize_telegram(block: Any) -> Dict[str, Any]:
    if isinstance(block, dict):
        chunks = block.get("chunks") or []
        if isinstance(chunks, list) and all(isinstance(c, str) for c in chunks) and chunks:
            # Remove MarkdownV2 escaping since bot sends without parse_mode
            unescaped_chunks = [_unescape_markdown_v2(c) for c in chunks]
            # Split each chunk if it's too long, then flatten
            final_chunks = []
            for chunk in unescaped_chunks:
                split_chunks = split_telegram_message(chunk)
                final_chunks.extend(split_chunks)
            character_count = sum(len(c) for c in final_chunks)
            return {"chunks": final_chunks, "character_count": character_count}
        
        # Check for mdv2 field (MarkdownV2 text from OpenAI)
        mdv2 = block.get("mdv2")
        if isinstance(mdv2, str) and mdv2.strip():
            # Remove escaping and split into chunks
            unescaped = _unescape_markdown_v2(mdv2)
            chunks = split_telegram_message(unescaped)
            character_count = sum(len(c) for c in chunks)
            return {"chunks": chunks, "character_count": character_count}
        
        # Check for text field
        text = block.get("text")
        if isinstance(text, str) and text.strip():
            return _prepare_telegram_from_text(text)
    elif isinstance(block, str) and block.strip():
        return _prepare_telegram_from_text(block)
    return {"chunks": [], "character_count": 0}


def _validate_output(parsed: Dict[str, Any], schema_text: Optional[str]) -> List[str]:
    errors: List[str] = []
    if not isinstance(parsed, dict):
        return ["Output is not a JSON object"]
    expected_keys = ["key_metrics", "anomalies", "insights", "recommendations", "telegram", "sheets"]
    for k in expected_keys:
        if k not in parsed:
            errors.append(f"Missing key: {k}")
    tg = parsed.get("telegram", {})
    if isinstance(tg, dict):
        has_chunks = isinstance(tg.get("chunks"), list)
        has_mdv2 = isinstance(tg.get("mdv2"), str)
        if not (has_chunks or has_mdv2):
            errors.append("telegram must have 'chunks' list or 'mdv2' string")
    else:
        errors.append("telegram must be an object")
    sh = parsed.get("sheets", {})
    if isinstance(sh, dict):
        if not isinstance(sh.get("headers"), list):
            errors.append("sheets.headers must be a list")
        if not isinstance(sh.get("rows"), list):
            errors.append("sheets.rows must be a list")
    else:
        errors.append("sheets must be an object")
    return errors


def run_analysis(
    client: GPTClient, data: Dict[str, Any], template_path: Optional[str] = None, validate: bool = False
) -> Dict[str, Any]:
    """
    Orchestrate LLM analysis:
    - compose messages
    - call LLM
    - parse JSON and prepare telegram/sheets outputs
    Returns dict with keys: messages, raw_response, json, telegram, sheets
    """
    import logging
    logger = logging.getLogger(__name__)
    
    messages = compose_messages(data, template_path)
    text = client.complete_messages(messages)
    
    # Save raw response to file for debugging
    try:
        import os
        debug_dir = os.path.join(os.path.dirname(__file__), "debug")
        os.makedirs(debug_dir, exist_ok=True)
        debug_file = os.path.join(debug_dir, "last_gpt_response.txt")
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(f"=== RAW GPT RESPONSE ({len(text)} chars) ===\n\n")
            f.write(text)
            f.write("\n\n=== END ===\n")
        logger.info(f"üíæ Saved raw response to {debug_file}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Failed to save debug file: {e}")

    # Handle explicit LLM errors (from client retries)
    result_error: Optional[str] = None
    if isinstance(text, str) and text.strip().startswith("ERROR:"):
        result_error = text.strip()

    parsed = _safe_json_extract(text) or {}

    telegram = _normalize_telegram(parsed.get("telegram")) if parsed else {"chunks": [], "character_count": 0}
    if not telegram["chunks"]:
        # Fallback: try extract text after 'OUTPUT_TG'
        lower = text.lower()
        idx = lower.find("output_tg")
        if idx != -1:
            tg_text = text[idx + len("output_tg") :].strip()
            if tg_text:
                telegram = _prepare_telegram_from_text(tg_text)

    # If still empty, and we have an LLM error, return a readable warning for TG
    if not telegram["chunks"] and result_error:
        # Check if it's a regional restriction error
        error_text = result_error
        error_lower = error_text.lower()
        
        if "unsupported_country_region_territory" in error_text or "–Ω–µ available in your region" in error_lower:
            error_text = (
                "‚ö†Ô∏è OpenAI API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –≤–∞—à–µ–º —Ä–µ–≥–∏–æ–Ω–µ.\n\n"
                "–î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π API endpoint:\n"
                "1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä –¥–ª—è OpenAI API\n"
                "2. –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, Azure OpenAI, Anyscale –∏ –¥—Ä.)\n"
                "3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_BASE_URL —Å –∞–¥—Ä–µ—Å–æ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ endpoint\n\n"
                f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏:\n{result_error}"
            )
        elif "connection error" in error_lower or "connection" in error_lower:
            # Connection error - provide helpful message
            error_text = (
                "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å API.\n\n"
                "–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Å–µ—Ä–≤–∏—Å–æ–º. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
                "‚Ä¢ –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º\n"
                "‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞\n"
                "‚Ä¢ –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –∏–ª–∏ DNS\n"
                "‚Ä¢ –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π—Ä–≤–æ–ª–æ–º\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                "1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n"
                "2. –ü–æ–¥–æ–∂–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É\n"
                "3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ—Ç–∏ –∏ –ø—Ä–æ–∫—Å–∏\n\n"
                f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{result_error}"
            )
        elif "timeout" in error_lower:
            error_text = (
                "‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç API.\n\n"
                "–°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –≤ —Ç–µ—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. "
                "–≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å —Å–µ—Ç—å—é –∏–ª–∏ –≤—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞.\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                "1. –ü–æ–¥–æ–∂–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É\n"
                "2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è\n\n"
                f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{result_error}"
            )
        else:
            # Generic error - provide helpful message
            error_text = (
                "‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM.\n\n"
                "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —Å–µ—Ä–≤–∏—Å—É –∞–Ω–∞–ª–∏–∑–∞. "
                "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.\n\n"
                f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{result_error}"
            )
        
        telegram = _prepare_telegram_from_text(error_text)

    sheets = parsed.get("sheets") if isinstance(parsed, dict) else None
    if not isinstance(sheets, dict):
        sheets = {"headers": [], "rows": []}
    if not isinstance(sheets.get("headers"), list):
        sheets["headers"] = []
    if not isinstance(sheets.get("rows"), list):
        sheets["rows"] = []

    result = {
        "messages": messages,
        "raw_response": text,
        "json": parsed,
        "telegram": telegram,
        "sheets": sheets,
    }
    if result_error:
        result["llm_error"] = result_error
    if validate:
        try:
            schema_text = get_output_json_schema(template_path)
        except Exception:
            schema_text = None
        result["validation_errors"] = _validate_output(parsed, schema_text)
    return result