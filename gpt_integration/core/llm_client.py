"""
Universal LLM Client –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ AI –º–æ–¥–µ–ª—è–º–∏
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç GPT-5.1 (OpenAI) –∏ Claude Sonnet 4.5 (Google Vertex AI)
"""
import os
import logging
from typing import List, Dict, Optional
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)


class UniversalLLMClient:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ LLM"""
    
    def __init__(self):
        # OpenAI –∫–ª–∏–µ–Ω—Ç
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        
        if self.openai_api_key:
            self.openai_client = AsyncOpenAI(
                api_key=self.openai_api_key,
                base_url=self.openai_base_url if self.openai_base_url.strip() else None
            )
            logger.info("‚úÖ OpenAI client initialized")
        else:
            self.openai_client = None
            logger.warning("‚ö†Ô∏è OpenAI API key not configured")
        
        # Google Vertex AI –¥–ª—è Claude (–±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø–æ–∑–∂–µ)
        self.vertex_project = os.getenv("GOOGLE_CLOUD_PROJECT")
        self.vertex_location = os.getenv("VERTEX_AI_LOCATION", "us-central1")
        self.vertex_client = None
        
        if self.vertex_project:
            try:
                from google.cloud import aiplatform
                aiplatform.init(project=self.vertex_project, location=self.vertex_location)
                logger.info(f"‚úÖ Vertex AI initialized: {self.vertex_project}/{self.vertex_location}")
                self.vertex_client = aiplatform
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Vertex AI not available: {e}")
    
    async def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> tuple[str, int]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è chat completion
        
        Args:
            model: ID –º–æ–¥–µ–ª–∏ (gpt-5.1, claude-sonnet-4.5)
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-2.0)
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        
        Returns:
            tuple[str, int]: (response_text, tokens_used)
        
        Raises:
            ValueError: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            RuntimeError: –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ
        """
        if model.startswith("gpt"):
            return await self._openai_completion(model, messages, temperature, max_tokens)
        elif model.startswith("claude"):
            return await self._claude_completion(model, messages, temperature, max_tokens)
        else:
            raise ValueError(f"Unsupported model: {model}")
    
    async def _openai_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> tuple[str, int]:
        """OpenAI completion (GPT-5.1)"""
        if not self.openai_client:
            raise RuntimeError("OpenAI client not initialized. Check OPENAI_API_KEY")
        
        try:
            logger.info(f"ü§ñ Calling OpenAI: model={model}, messages={len(messages)}")
            
            response = await self.openai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            response_text = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            logger.info(f"‚úÖ OpenAI response: {len(response_text)} chars, {tokens_used} tokens")
            
            return (response_text, tokens_used)
            
        except Exception as e:
            logger.error(f"‚ùå OpenAI API error: {str(e)}")
            raise RuntimeError(f"OpenAI API error: {str(e)}")
    
    async def _claude_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> tuple[str, int]:
        """Claude completion —á–µ—Ä–µ–∑ Google Vertex AI"""
        if not self.vertex_client:
            # Fallback –Ω–∞ OpenAI –µ—Å–ª–∏ Vertex AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            logger.warning(f"‚ö†Ô∏è Vertex AI not available, falling back to GPT-5.1")
            return await self._openai_completion("gpt-5.1", messages, temperature, max_tokens)
        
        try:
            logger.info(f"ü§ñ Calling Claude via Vertex AI: model={model}, messages={len(messages)}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è Claude
            system_message = None
            claude_messages = []
            
            for msg in messages:
                if msg["role"] == "system":
                    system_message = msg["content"]
                else:
                    claude_messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Vertex AI Anthropic API
            from anthropic import AnthropicVertex
            
            client = AnthropicVertex(
                project_id=self.vertex_project,
                region=self.vertex_location
            )
            
            # –ú–∞–ø–ø–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π
            vertex_model = "claude-sonnet-4-5@20250514" if model == "claude-sonnet-4.5" else model
            
            response = await client.messages.create(
                model=vertex_model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_message,
                messages=claude_messages
            )
            
            response_text = response.content[0].text
            tokens_used = response.usage.input_tokens + response.usage.output_tokens
            
            logger.info(f"‚úÖ Claude response: {len(response_text)} chars, {tokens_used} tokens")
            
            return (response_text, tokens_used)
            
        except Exception as e:
            logger.error(f"‚ùå Claude/Vertex AI error: {str(e)}")
            # Fallback –Ω–∞ OpenAI
            logger.warning(f"‚ö†Ô∏è Falling back to GPT-5.1 due to error")
            return await self._openai_completion("gpt-5.1", messages, temperature, max_tokens)


# Singleton instance
llm_client = UniversalLLMClient()
