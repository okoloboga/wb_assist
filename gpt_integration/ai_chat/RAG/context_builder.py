"""
Context Builder - –º–æ–¥—É–ª—å –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.

–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
–¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç LLM.
"""

import os
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class ContextBuilder:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    
    –ü—Ä–æ—Ü–µ—Å—Å —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è:
    1. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
    2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
    3. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç
    4. –û–±—Ä–µ–∑–∫–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
    """
    
    def __init__(self, max_length: Optional[int] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–ª–¥–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        Args:
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (–∏–∑ env –∏–ª–∏ default)
        """
        self.max_length = max_length or int(
            os.getenv("RAG_CONTEXT_MAX_LENGTH", "3000")
        )
        
        # –ù–∞–∑–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        self.type_names = {
            'order': '–ó–ê–ö–ê–ó–´',
            'product': '–¢–û–í–ê–†–´',
            'stock': '–û–°–¢–ê–¢–ö–ò',
            'review': '–û–¢–ó–´–í–´',
            'sale': '–ü–†–û–î–ê–ñ–ò'
        }
        
        # –ü–æ—Ä—è–¥–æ–∫ –≤—ã–≤–æ–¥–∞ —Ç–∏–ø–æ–≤ (–æ—Ç –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –∫ –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–º)
        self.type_order = ['product', 'order', 'sale', 'stock', 'review']
    
    def group_by_type(self, chunks: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –ø–æ —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö.
        
        –¢–∞–∫–∂–µ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –ø–æ similarity (–æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É).
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á - —Ç–∏–ø —á–∞–Ω–∫–∞, –∑–Ω–∞—á–µ–Ω–∏–µ - —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
        """
        grouped = {}
        
        for chunk in chunks:
            chunk_type = chunk.get('chunk_type', 'unknown')
            if chunk_type not in grouped:
                grouped[chunk_type] = []
            grouped[chunk_type].append(chunk)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –ø–æ similarity (–æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É)
        for chunk_type in grouped:
            grouped[chunk_type].sort(
                key=lambda x: x.get('similarity', 0),
                reverse=True
            )
        
        logger.debug(f"üìä Grouping: {len(grouped)} types, total chunks: {len(chunks)}")
        
        return grouped
    
    def deduplicate(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ source_table –∏ source_id.
        
        –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç, –æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π (—Å –±–æ–ª—å—à–∏–º similarity).
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
        """
        seen = {}
        unique_chunks = []
        
        for chunk in chunks:
            source_table = chunk.get('source_table')
            source_id = chunk.get('source_id')
            
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å, –µ—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            if source_table is None or source_id is None:
                unique_chunks.append(chunk)
                continue
            
            key = (source_table, source_id)
            chunk_similarity = chunk.get('similarity', 0)
            
            if key not in seen:
                # –ü–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                seen[key] = len(unique_chunks)
                unique_chunks.append(chunk)
            else:
                # –î—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω - —Å—Ä–∞–≤–Ω–∏—Ç—å similarity
                existing_index = seen[key]
                existing_chunk = unique_chunks[existing_index]
                existing_similarity = existing_chunk.get('similarity', 0)
                
                if chunk_similarity > existing_similarity:
                    # –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
                    unique_chunks[existing_index] = chunk
                    logger.debug(
                        f"üîÑ –ó–∞–º–µ–Ω–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {key}, "
                        f"similarity {existing_similarity:.2f} -> {chunk_similarity:.2f}"
                    )
        
        removed_count = len(chunks) - len(unique_chunks)
        if removed_count > 0:
            logger.info(f"üóëÔ∏è Removed {removed_count} duplicates from {len(chunks)} chunks")
        
        return unique_chunks
    
    def format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
        
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –ø–æ —Ç–∏–ø–∞–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Ö –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç
        —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º.
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        if not chunks:
            return ""
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–ø–∞–º
        grouped = self.group_by_type(chunks)
        
        # –ù–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_parts = ["=== –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –î–ê–ù–ù–´–• ===\n"]
        
        # –í—ã–≤–µ—Å—Ç–∏ —á–∞–Ω–∫–∏ –≤ –ø–æ—Ä—è–¥–∫–µ type_order
        for chunk_type in self.type_order:
            if chunk_type in grouped:
                type_name = self.type_names.get(chunk_type, chunk_type.upper())
                context_parts.append(f"## {type_name}\n")
                
                for chunk in grouped[chunk_type]:
                    chunk_text = chunk.get('chunk_text', '').strip()
                    if chunk_text:
                        # –î–æ–±–∞–≤–∏—Ç—å similarity –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                        similarity = chunk.get('similarity', 0)
                        context_parts.append(f"- {chunk_text}")
                        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å similarity –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏:
                        # context_parts.append(f"- {chunk_text} [similarity: {similarity:.2f}]")
                
                context_parts.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏
        
        # –í—ã–≤–µ—Å—Ç–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã (–µ—Å–ª–∏ –µ—Å—Ç—å), –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ type_order
        for chunk_type, chunk_list in grouped.items():
            if chunk_type not in self.type_order:
                type_name = self.type_names.get(chunk_type, chunk_type.upper())
                context_parts.append(f"## {type_name}\n")
                
                for chunk in chunk_list:
                    chunk_text = chunk.get('chunk_text', '').strip()
                    if chunk_text:
                        context_parts.append(f"- {chunk_text}")
                
                context_parts.append("")
        
        context_text = "\n".join(context_parts)
        
        logger.debug(f"üìù Formatted context: {len(context_text)} characters")
        
        return context_text
    
    def truncate_context(self, context: str, max_length: int) -> str:
        """
        –û–±—Ä–µ–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã.
        
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —É–¥–∞–ª—è—è –º–µ–Ω–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ (–≤ –∫–æ–Ω—Ü–µ).
        
        Args:
            context: –¢–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            
        Returns:
            –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω, –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä)
        """
        if len(context) <= max_length:
            return context
        
        logger.info(
            f"‚úÇÔ∏è –û–±—Ä–µ–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(context)} -> {max_length} —Å–∏–º–≤–æ–ª–æ–≤"
        )
        
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —Å—Ç—Ä–æ–∫–∏
        lines = context.split('\n')
        truncated_lines = []
        current_length = 0
        
        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_length = len("=== –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –î–ê–ù–ù–´–• ===\n")
        current_length = header_length
        truncated_lines.append("=== –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –î–ê–ù–ù–´–• ===\n")
        
        # –î–æ–±–∞–≤–ª—è—Ç—å —Å—Ç—Ä–æ–∫–∏ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞
        for line in lines[1:]:  # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω)
            line_length = len(line) + 1  # +1 –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
            
            if current_length + line_length <= max_length:
                truncated_lines.append(line)
                current_length += line_length
            else:
                # –ù–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è - –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
                break
        
        # –î–æ–±–∞–≤–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –æ–±—Ä–µ–∑–∫–∏, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω
        if len(truncated_lines) < len(lines):
            # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø–æ–º–µ—â–∞–µ—Ç—Å—è
            indicator = "\n... (–∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã)"
            if current_length + len(indicator) <= max_length:
                truncated_lines.append(indicator)
            else:
                # –ï—Å–ª–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è, —É–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –∏ –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
                if truncated_lines:
                    truncated_lines.pop()
                truncated_lines.append("... (–∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω)")
        
        truncated_text = "\n".join(truncated_lines)
        
        logger.info(
            f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω: {len(truncated_text)} —Å–∏–º–≤–æ–ª–æ–≤ "
            f"(–±—ã–ª–æ {len(context)})"
        )
        
        return truncated_text
    
    def build_context(
        self,
        chunks: List[Dict[str, Any]],
        max_length: Optional[int] = None
    ) -> str:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —ç—Ç–∞–ø—ã:
        1. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        3. –û–±—Ä–µ–∑–∫–∞
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç VectorSearch)
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç default)
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç
        """
        if not chunks:
            logger.info("‚ö†Ô∏è No chunks to build context from")
            return ""
        
        max_length = max_length or self.max_length
        
        logger.info(
            f"üî® –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤ "
            f"(max_length={max_length})"
        )
        
        # 1. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_chunks = self.deduplicate(chunks)
        
        # 2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        context = self.format_context(unique_chunks)
        
        # 3. –û–±—Ä–µ–∑–∫–∞
        context = self.truncate_context(context, max_length)
        
        logger.info(
            f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤, "
            f"{len(unique_chunks)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"
        )
        
        return context
















