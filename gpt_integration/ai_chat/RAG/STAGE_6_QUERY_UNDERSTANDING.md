# –≠—Ç–∞–ø 6: Query Understanding (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

## üìã –û–±–∑–æ—Ä —ç—Ç–∞–ø–∞

**–¶–µ–ª—å:** –£–ª—É—á—à–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –∑–∞ —Å—á–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** 1-2 –¥–Ω—è

**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** –≠—Ç–∞–ø 3 (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ú–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞.

---

## üéØ –ó–∞–¥–∞—á–∏ —ç—Ç–∞–ø–∞

### –ó–∞–¥–∞—á–∞ 6.1: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥—É–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤

**–§–∞–π–ª:** `gpt_integration/ai_chat/rag/query_processor.py`

**–ö–ª–∞—Å—Å:** `QueryProcessor`

**–ú–µ—Ç–æ–¥—ã:**
- `classify_intent(query_text)` ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è
- `extract_entities(query_text)` ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
- `get_chunk_types_for_intent(intent)` ‚Äî –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤

---

### –ó–∞–¥–∞—á–∞ 6.2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö):**

```python
def classify_intent(self, query_text: str) -> str:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    query_lower = query_text.lower()
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
    sales_keywords = ['–ø—Ä–æ–¥–∞–∂', '–≤—ã–∫—É–ø', '–∑–∞–∫–∞–∑', '–ø—Ä–æ–¥–∞–ª', '–∫—É–ø–∏–ª']
    products_keywords = ['—Ç–æ–≤–∞—Ä', '–ø—Ä–æ–¥—É–∫—Ç', '–∫–∞—Ä—Ç–æ—á–∫–∞', '–∞—Ä—Ç–∏–∫—É–ª']
    stocks_keywords = ['–æ—Å—Ç–∞—Ç–æ–∫', '—Å–∫–ª–∞–¥', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–µ—Å—Ç—å –≤ –Ω–∞–ª–∏—á–∏–∏']
    reviews_keywords = ['–æ—Ç–∑—ã–≤', '—Ä–µ–π—Ç–∏–Ω–≥', '–æ—Ü–µ–Ω–∫–∞', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π']
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    scores = {
        'sales_analytics': sum(1 for kw in sales_keywords if kw in query_lower),
        'products': sum(1 for kw in products_keywords if kw in query_lower),
        'stocks': sum(1 for kw in stocks_keywords if kw in query_lower),
        'reviews': sum(1 for kw in reviews_keywords if kw in query_lower)
    }
    
    # –í–µ—Ä–Ω—É—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—á–µ—Ç–æ–º
    max_score = max(scores.values())
    if max_score > 0:
        return max(scores, key=scores.get)
    
    return 'general'  # –û–±—â–µ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
```

**–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM):**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è, –Ω–æ –¥–æ—Ä–æ–∂–µ)
- –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- –í—ã–∑–≤–∞—Ç—å OpenAI API

---

### –ó–∞–¥–∞—á–∞ 6.3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**

```python
def extract_entities(self, query_text: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∑–∞–ø—Ä–æ—Å–∞."""
    import re
    
    entities = {
        'nm_ids': [],
        'dates': [],
        'categories': [],
        'brands': []
    }
    
    # –ü–æ–∏—Å–∫ nm_id (—á–∏—Å–ª–∞ –ø–æ—Å–ª–µ "nm_id", "–∞—Ä—Ç–∏–∫—É–ª", "–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞")
    nm_id_pattern = r'(?:nm_id|–∞—Ä—Ç–∏–∫—É–ª|–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞)[\s:]*(\d+)'
    nm_ids = re.findall(nm_id_pattern, query_text, re.IGNORECASE)
    entities['nm_ids'] = [int(nm_id) for nm_id in nm_ids]
    
    # –ü–æ–∏—Å–∫ –¥–∞—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    date_keywords = {
        '—Å–µ–≥–æ–¥–Ω—è': 0,
        '–≤—á–µ—Ä–∞': 1,
        '–Ω–µ–¥–µ–ª—è': 7,
        '–º–µ—Å—è—Ü': 30
    }
    for keyword, days in date_keywords.items():
        if keyword in query_text.lower():
            entities['dates'].append(keyword)
    
    return entities
```

---

### –ó–∞–¥–∞—á–∞ 6.4: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**

```python
def get_chunk_types_for_intent(self, intent: str) -> Optional[List[str]]:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤ –¥–ª—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è."""
    mapping = {
        'sales_analytics': ['sale', 'order'],
        'products': ['product', 'review'],
        'stocks': ['stock'],
        'reviews': ['review'],
        'general': None  # –í—Å–µ —Ç–∏–ø—ã
    }
    
    return mapping.get(intent)
```

---

### –ó–∞–¥–∞—á–∞ 6.5: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ `VectorSearch.search_relevant_chunks()`:**

```python
# –í –º–µ—Ç–æ–¥–µ search_relevant_chunks():
query_processor = QueryProcessor()
intent = query_processor.classify_intent(query_text)
chunk_types = query_processor.get_chunk_types_for_intent(intent)

# –ü–µ—Ä–µ–¥–∞—Ç—å chunk_types –≤ –ø–æ–∏—Å–∫
chunks = self.search(
    query_embedding=query_embedding,
    cabinet_id=cabinet_id,
    chunk_types=chunk_types,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã
    limit=max_chunks
)
```

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏

- ‚úÖ –ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ–∑–¥–∞–Ω
- ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–æ–∏—Å–∫–æ–º —Ä–∞–±–æ—Ç–∞–µ—Ç

---

## üéØ –£–ª—É—á—à–µ–Ω–∏—è (–±—É–¥—É—â–µ–µ)

- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- NER (Named Entity Recognition) –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏

---

**–í–µ—Ä—Å–∏—è:** 1.0.0  
**–î–∞—Ç–∞:** 2025-01-XX  
**–°—Ç–∞—Ç—É—Å:** –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –≠—Ç–∞–ø–∞ 6

