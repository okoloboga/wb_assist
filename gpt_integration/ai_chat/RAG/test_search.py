"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è RAG –ø–æ–∏—Å–∫–∞.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python -m gpt_integration.ai_chat.RAG.test_search <cabinet_id> "<query>"
"""

import asyncio
import sys
import logging
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
root_dir = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(root_dir))

from gpt_integration.ai_chat.RAG.vector_search import VectorSearch

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_search(cabinet_id: int, query: str):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞"""
    logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É—é –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–±–∏–Ω–µ—Ç–∞ {cabinet_id}...")
    logger.info(f"   –ó–∞–ø—Ä–æ—Å: '{query}'")
    
    searcher = VectorSearch()
    results = searcher.search_relevant_chunks(
        query_text=query,
        cabinet_id=cabinet_id,
        max_chunks=5
    )
    
    if results:
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤:")
        for idx, chunk in enumerate(results, 1):
            logger.info(
                f"   [{idx}] similarity={chunk['similarity']:.4f}, "
                f"type={chunk['chunk_type']}, "
                f"text_preview='{chunk['chunk_text'][:80]}...'"
            )
    else:
        logger.warning("‚ö†Ô∏è –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return results


def main():
    if len(sys.argv) < 3:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python -m gpt_integration.ai_chat.RAG.test_search <cabinet_id> \"<query>\"")
        print("–ü—Ä–∏–º–µ—Ä: python -m gpt_integration.ai_chat.RAG.test_search 2 \"—Å–∫–æ–ª—å–∫–æ –∑–∞–∫–∞–∑–æ–≤ –±—ã–ª–æ –≤—á–µ—Ä–∞\"")
        sys.exit(1)
    
    cabinet_id = int(sys.argv[1])
    query = sys.argv[2]
    
    test_search(cabinet_id, query)


if __name__ == "__main__":
    main()

