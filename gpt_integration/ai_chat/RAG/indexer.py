"""
RAG Indexer - —Å–µ—Ä–≤–∏—Å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î.

–ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î, —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤,
–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î.
"""

import os
import asyncio
import hashlib
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from gpt_integration.comet_client import comet_client

from .database import RAGSessionLocal
from .models import RAGMetadata, RAGEmbedding, RAGIndexStatus
from ..tools.db_pool import get_asyncpg_pool

logger = logging.getLogger(__name__)


class RAGIndexer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î.
    
    –ü—Ä–æ—Ü–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:
    1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î
    2. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
    3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ OpenAI API
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
    """
    
    def __init__(
        self,
        batch_size: Optional[int] = None
    ):
        """
        Initializes the indexer.
        The client for creating embeddings is now the centralized CometClient.
        """
        self.batch_size = batch_size or int(os.getenv("RAG_EMBEDDING_BATCH_SIZE", "100"))

    @staticmethod
    def calculate_chunk_hash(chunk_text: str) -> str:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å SHA256 hash –æ—Ç chunk_text.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è hash-based change detection:
        - –ï—Å–ª–∏ hash –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, —Ç–æ chunk_text –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
        - –ú–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (—ç–∫–æ–Ω–æ–º–∏—è API)

        Args:
            chunk_text: –¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞

        Returns:
            SHA256 hash –≤ hex —Ñ–æ—Ä–º–∞—Ç–µ (64 —Å–∏–º–≤–æ–ª–∞)
        """
        return hashlib.sha256(chunk_text.encode('utf-8')).hexdigest()
        
    async def extract_data_from_main_db(self, cabinet_id: int) -> Dict[str, List[Dict[str, Any]]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î –¥–ª—è –∫–∞–±–∏–Ω–µ—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncpg –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î.
        –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î —á–µ—Ä–µ–∑ DATABASE_URL (–Ω–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π).
        
        Args:
            cabinet_id: ID –∫–∞–±–∏–Ω–µ—Ç–∞ Wildberries
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∏–ø–∞–º:
            {
                'orders': [...],
                'products': [...],
                'stocks': [...],
                'reviews': [...],
                'sales': [...]
            }
        """
        # –ü–æ–ª—É—á–∏—Ç—å –ø—É–ª –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π –∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î
        pool = await get_asyncpg_pool()
        
        data = {
            'orders': [],
            'products': [],
            'stocks': [],
            'reviews': [],
            'sales': []
        }
        
        try:
            async with pool.acquire() as conn:
                # 1. –ó–∞–∫–∞–∑—ã
                orders = await conn.fetch("""
                    SELECT id, order_id, nm_id, name, size, price, total_price, 
                           order_date, status
                    FROM wb_orders
                    WHERE cabinet_id = $1
                      AND order_date >= NOW() - INTERVAL '90 days'
                    ORDER BY order_date DESC
                """, cabinet_id)
                data['orders'] = [dict(row) for row in orders]
                
                # 2. –¢–æ–≤–∞—Ä—ã
                products = await conn.fetch("""
                    SELECT nm_id, name, brand, category, price, rating, reviews_count
                    FROM wb_products
                    WHERE cabinet_id = $1
                      AND is_active = true
                """, cabinet_id)
                data['products'] = [dict(row) for row in products]
                
                # 3. –û—Å—Ç–∞—Ç–∫–∏
                stocks = await conn.fetch("""
                    SELECT nm_id, size, warehouse_name, quantity, name
                    FROM wb_stocks
                    WHERE cabinet_id = $1
                      AND quantity > 0
                """, cabinet_id)
                data['stocks'] = [dict(row) for row in stocks]
                
                # 4. –û—Ç–∑—ã–≤—ã
                reviews = await conn.fetch("""
                    SELECT id, nm_id, rating, text, created_at
                    FROM wb_reviews
                    WHERE cabinet_id = $1
                      AND created_at >= NOW() - INTERVAL '90 days'
                    ORDER BY created_at DESC
                """, cabinet_id)
                data['reviews'] = [dict(row) for row in reviews]
                
                # 5. –ü—Ä–æ–¥–∞–∂–∏ (JOIN —Å wb_products –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞)
                sales = await conn.fetch("""
                    SELECT s.id, s.nm_id, s.type, s.sale_date, s.amount,
                           COALESCE(p.name, s.product_name) as product_name
                    FROM wb_sales s
                    LEFT JOIN wb_products p ON s.nm_id = p.nm_id AND s.cabinet_id = p.cabinet_id
                    WHERE s.cabinet_id = $1
                      AND s.sale_date >= NOW() - INTERVAL '90 days'
                    ORDER BY s.sale_date DESC
                """, cabinet_id)
                data['sales'] = [dict(row) for row in sales]
                
        except Exception as e:
            logger.error(f"‚ùå Error extracting data for cabinet {cabinet_id}: {e}")
            raise
        
        logger.info(
            f"üìä Extracted data for cabinet {cabinet_id}: "
            f"orders={len(data['orders'])}, products={len(data['products'])}, "
            f"stocks={len(data['stocks'])}, reviews={len(data['reviews'])}, "
            f"sales={len(data['sales'])}"
        )
        
        return data

    async def extract_data_by_ids(
        self,
        cabinet_id: int,
        changed_ids: Dict[str, List[int]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–ø–∏—Å–∫—É ID (Event-driven indexing).

        –í–º–µ—Å—Ç–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ timestamp, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º ID.
        –ì–æ—Ä–∞–∑–¥–æ –±—ã—Å—Ç—Ä–µ–µ —Ç.–∫. –∏—Å–ø–æ–ª—å–∑—É–µ—Ç indexed lookup.

        Args:
            cabinet_id: ID –∫–∞–±–∏–Ω–µ—Ç–∞
            changed_ids: –î–µ–ª—å—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –æ—Ç WB sync
                {
                    "orders": [12345, 12346],
                    "products": [98765],
                    "stocks": [11111, 11112],
                    "reviews": [55555],
                    "sales": [77777]
                }

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∏–ø–∞–º (—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π)
        """
        pool = await get_asyncpg_pool()

        data = {
            'orders': [],
            'products': [],
            'stocks': [],
            'reviews': [],
            'sales': []
        }

        try:
            async with pool.acquire() as conn:
                # 1. –ó–∞–∫–∞–∑—ã
                if changed_ids.get('orders'):
                    orders = await conn.fetch("""
                        SELECT id, order_id, nm_id, name, size, price, total_price,
                               order_date, status
                        FROM wb_orders
                        WHERE cabinet_id = $1
                          AND id = ANY($2::bigint[])
                          AND order_date >= NOW() - INTERVAL '90 days'
                        ORDER BY order_date DESC
                    """, cabinet_id, changed_ids['orders'])
                    data['orders'] = [dict(row) for row in orders]

                # 2. –¢–æ–≤–∞—Ä—ã
                if changed_ids.get('products'):
                    products = await conn.fetch("""
                        SELECT nm_id, name, brand, category, price, rating, reviews_count
                        FROM wb_products
                        WHERE cabinet_id = $1
                          AND id = ANY($2::bigint[])
                          AND is_active = true
                    """, cabinet_id, changed_ids['products'])
                    data['products'] = [dict(row) for row in products]

                # 3. –û—Å—Ç–∞—Ç–∫–∏
                if changed_ids.get('stocks'):
                    stocks = await conn.fetch("""
                        SELECT nm_id, size, warehouse_name, quantity, name
                        FROM wb_stocks
                        WHERE cabinet_id = $1
                          AND id = ANY($2::bigint[])
                          AND quantity > 0
                    """, cabinet_id, changed_ids['stocks'])
                    data['stocks'] = [dict(row) for row in stocks]

                # 4. –û—Ç–∑—ã–≤—ã
                if changed_ids.get('reviews'):
                    reviews = await conn.fetch("""
                        SELECT id, nm_id, rating, text, created_at
                        FROM wb_reviews
                        WHERE cabinet_id = $1
                          AND id = ANY($2::bigint[])
                          AND created_at >= NOW() - INTERVAL '90 days'
                        ORDER BY created_at DESC
                    """, cabinet_id, changed_ids['reviews'])
                    data['reviews'] = [dict(row) for row in reviews]

                # 5. –ü—Ä–æ–¥–∞–∂–∏ (JOIN —Å wb_products –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞)
                if changed_ids.get('sales'):
                    sales = await conn.fetch("""
                        SELECT s.id, s.nm_id, s.type, s.sale_date, s.amount,
                               COALESCE(p.name, s.product_name) as product_name
                        FROM wb_sales s
                        LEFT JOIN wb_products p ON s.nm_id = p.nm_id AND s.cabinet_id = p.cabinet_id
                        WHERE s.cabinet_id = $1
                          AND s.id = ANY($2::bigint[])
                          AND s.sale_date >= NOW() - INTERVAL '90 days'
                        ORDER BY s.sale_date DESC
                    """, cabinet_id, changed_ids['sales'])
                    data['sales'] = [dict(row) for row in sales]

        except Exception as e:
            logger.error(f"Error extracting data by IDs for cabinet {cabinet_id}: {e}")
            raise

        logger.info(
            f"Extracted data by IDs for cabinet {cabinet_id}: "
            f"orders={len(data['orders'])}, products={len(data['products'])}, "
            f"stocks={len(data['stocks'])}, reviews={len(data['reviews'])}, "
            f"sales={len(data['sales'])}"
        )

        return data

    def _create_order_chunk(self, order: Dict[str, Any], product_name: Optional[str] = None) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –¥–ª—è –∑–∞–∫–∞–∑–∞."""
        name = product_name or order.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–≤–∞—Ä')
        order_id = order.get('order_id', order.get('id', 'N/A'))
        order_date = order.get('order_date')
        if isinstance(order_date, datetime):
            order_date = order_date.strftime('%Y-%m-%d')
        elif order_date:
            order_date = str(order_date)
        else:
            order_date = 'N/A'
        
        price = order.get('price', 0) or 0
        if not isinstance(price, (int, float)):
            price = 0
        
        chunk_text = (
            f"–ó–∞–∫–∞–∑ #{order_id} –æ—Ç {order_date}: "
            f"—Ç–æ–≤–∞—Ä '{name}' (nm_id: {order.get('nm_id', 'N/A')}), "
            f"—Ä–∞–∑–º–µ—Ä {order.get('size', 'N/A')}, "
            f"—Ü–µ–Ω–∞ {price:.2f}‚ÇΩ, "
            f"—Å—Ç–∞—Ç—É—Å: {order.get('status', 'N/A')}"
        )
        
        return {
            'chunk_type': 'order',
            'source_table': 'wb_orders',
            'source_id': order.get('id'),
            'chunk_text': chunk_text
        }
    
    def _create_product_chunk(self, product: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–∞."""
        name = product.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–≤–∞—Ä')
        brand = product.get('brand', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –±—Ä–µ–Ω–¥')
        category = product.get('category', '–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
        rating = product.get('rating', 0) or 0
        reviews_count = product.get('reviews_count', 0) or 0
        price = product.get('price', 0) or 0
        nm_id = product.get('nm_id', 'N/A')
        
        if not isinstance(rating, (int, float)):
            rating = 0
        if not isinstance(reviews_count, (int, float)):
            reviews_count = 0
        if not isinstance(price, (int, float)):
            price = 0
        
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        # –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
        chunk_text = (
            f"–¢–æ–≤–∞—Ä –ø—Ä–æ–¥—É–∫—Ç '{name}' –∞—Ä—Ç–∏–∫—É–ª nm_id {nm_id}. "
            f"–ë—Ä–µ–Ω–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å: {brand}. "
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞: {category}. "
            f"–†–µ–π—Ç–∏–Ω–≥ –æ—Ü–µ–Ω–∫–∞: {rating:.1f} –∏–∑ 5. "
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤: {reviews_count}. "
            f"–¶–µ–Ω–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å: {price:.2f} —Ä—É–±–ª–µ–π. "
            f"–ê—Ä—Ç–∏–∫—É–ª nm_id: {nm_id}"
        )
        
        return {
            'chunk_type': 'product',
            'source_table': 'wb_products',
            'source_id': nm_id,
            'chunk_text': chunk_text
        }
    
    def _create_stock_chunk(self, stock: Dict[str, Any], product_name: Optional[str] = None) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –¥–ª—è –æ—Å—Ç–∞—Ç–∫–∞ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞."""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: product_name –∏–∑ —Å–ª–æ–≤–∞—Ä—è -> name –∏–∑ –ë–î -> fallback
        name = product_name or stock.get('name') or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–≤–∞—Ä'
        warehouse = stock.get('warehouse_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∫–ª–∞–¥')
        quantity = stock.get('quantity', 0) or 0
        size = stock.get('size', 'N/A')
        nm_id = stock.get('nm_id', 'N/A')

        if not isinstance(quantity, (int, float)):
            quantity = 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Å—Ç–∞—Ç–∫–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        status_keywords = []
        if quantity == 0:
            status_keywords = ["–Ω—É–ª–µ–≤–æ–π –æ—Å—Ç–∞—Ç–æ–∫", "—Ç–æ–≤–∞—Ä –∑–∞–∫–æ–Ω—á–∏–ª—Å—è", "–°–†–û–ß–ù–û –ø–æ–ø–æ–ª–Ω–∏—Ç—å", "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Å—Ç–∞—Ç–æ–∫"]
        elif quantity <= 5:
            status_keywords = ["–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Å—Ç–∞—Ç–æ–∫", "–æ—á–µ–Ω—å –º–∞–ª–æ", "—Å—Ä–æ—á–Ω–æ –ø–æ–ø–æ–ª–Ω–∏—Ç—å", "—Ç—Ä–µ–±—É–µ—Ç –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è"]
        elif quantity <= 10:
            status_keywords = ["–Ω–∏–∑–∫–∏–π –æ—Å—Ç–∞—Ç–æ–∫", "–º–∞–ª–æ —Ç–æ–≤–∞—Ä–∞", "–Ω—É–∂–Ω–æ –ø–æ–ø–æ–ª–Ω–∏—Ç—å", "—Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è"]
        elif quantity <= 20:
            status_keywords = ["–Ω–µ–≤—ã—Å–æ–∫–∏–π –æ—Å—Ç–∞—Ç–æ–∫", "—Å—Ç–æ–∏—Ç –ø–æ–ø–æ–ª–Ω–∏—Ç—å", "–∑–∞–ø–∞—Å –Ω–∞ –∏—Å—Ö–æ–¥–µ"]
        else:
            status_keywords = ["–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –æ—Å—Ç–∞—Ç–æ–∫", "–∑–∞–ø–∞—Å –≤ –Ω–æ—Ä–º–µ"]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        chunk_text = (
            f"–û—Å—Ç–∞—Ç–æ–∫ –∑–∞–ø–∞—Å —Ç–æ–≤–∞—Ä–∞ '{name}' –∞—Ä—Ç–∏–∫—É–ª nm_id {nm_id}: "
            f"—Ä–∞–∑–º–µ—Ä {size}, —Å–∫–ª–∞–¥ {warehouse}, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ {int(quantity)} —à—Ç—É–∫. "
            f"–°—Ç–∞—Ç—É—Å: {', '.join(status_keywords)}."
        )

        return {
            'chunk_type': 'stock',
            'source_table': 'wb_stocks',
            'source_id': nm_id,
            'chunk_text': chunk_text
        }
    
    def _create_review_chunk(self, review: Dict[str, Any], product_name: Optional[str] = None) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –¥–ª—è –æ—Ç–∑—ã–≤–∞."""
        name = product_name or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–≤–∞—Ä'
        rating = review.get('rating', 0) or 0
        text = review.get('text', '–ë–µ–∑ —Ç–µ–∫—Å—Ç–∞') or '–ë–µ–∑ —Ç–µ–∫—Å—Ç–∞'
        created_at = review.get('created_at')
        
        if isinstance(created_at, datetime):
            created_at = created_at.strftime('%Y-%m-%d')
        elif created_at:
            created_at = str(created_at)
        else:
            created_at = 'N/A'
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∑—ã–≤–∞ (—á—Ç–æ–±—ã —á–∞–Ω–∫ –Ω–µ –±—ã–ª —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º)
        if len(text) > 200:
            text = text[:200] + '...'
        
        if not isinstance(rating, (int, float)):
            rating = 0
        
        chunk_text = (
            f"–û—Ç–∑—ã–≤ –Ω–∞ —Ç–æ–≤–∞—Ä '{name}' (nm_id: {review.get('nm_id', 'N/A')}): "
            f"—Ä–µ–π—Ç–∏–Ω–≥ {int(rating)}‚≠ê, –¥–∞—Ç–∞: {created_at}, —Ç–µ–∫—Å—Ç: '{text}'"
        )
        
        return {
            'chunk_type': 'review',
            'source_table': 'wb_reviews',
            'source_id': review.get('id'),
            'chunk_text': chunk_text
        }
    
    def _create_sale_chunk(self, sale: Dict[str, Any], product_name: Optional[str] = None) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –¥–ª—è –ø—Ä–æ–¥–∞–∂–∏."""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: product_name –∏–∑ —Å–ª–æ–≤–∞—Ä—è -> product_name –∏–∑ –ë–î -> fallback
        name = product_name or sale.get('product_name') or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–≤–∞—Ä'
        sale_type = sale.get('type', 'N/A')
        sale_date = sale.get('sale_date')

        if isinstance(sale_date, datetime):
            sale_date = sale_date.strftime('%Y-%m-%d')
        elif sale_date:
            sale_date = str(sale_date)
        else:
            sale_date = 'N/A'

        amount = sale.get('amount', 0) or 0
        if not isinstance(amount, (int, float)):
            amount = 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫—É —Ç–∏–ø–∞ –ø—Ä–æ–¥–∞–∂–∏ (–≤—ã–¥–µ–ª—è–µ–º –í–´–ö–£–ü vs –í–û–ó–í–†–ê–¢)
        if sale_type == "buyout":
            type_label = "–í–´–ö–£–ü"
        elif sale_type == "return":
            type_label = "–í–û–ó–í–†–ê–¢"
        else:
            type_label = sale_type.upper()

        # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: —Ç–∏–ø –∏ –¥–∞—Ç–∞ –≤–Ω–∞—á–∞–ª–µ –¥–ª—è –ª—É—á—à–µ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏–∏
        chunk_text = (
            f"{type_label} –æ—Ç {sale_date}: —Ç–æ–≤–∞—Ä '{name}' (nm_id: {sale.get('nm_id', 'N/A')}), "
            f"—Å—É–º–º–∞: {amount:.2f}‚ÇΩ"
        )

        return {
            'chunk_type': 'sale',
            'source_table': 'wb_sales',
            'source_id': sale.get('id'),
            'chunk_text': chunk_text
        }
    
    def create_chunks(self, data: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        –°–æ–∑–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –Ω–∞–∑–≤–∞–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö —á–∞–Ω–∫–∞—Ö.
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∏–ø–∞–º
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —á–∞–Ω–∫–∞–º–∏:
            [
                {
                    'chunk_type': 'order',
                    'source_table': 'wb_orders',
                    'source_id': 123,
                    'chunk_text': '–ó–∞–∫–∞–∑ #12345...'
                },
                ...
            ]
        """
        chunks = []
        
        # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å –Ω–∞–∑–≤–∞–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤ (nm_id -> name)
        product_names = {}
        for product in data.get('products', []):
            nm_id = product.get('nm_id')
            if nm_id:
                product_names[nm_id] = product.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–≤–∞—Ä')
        
        # –°–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
        for order in data.get('orders', []):
            nm_id = order.get('nm_id')
            product_name = product_names.get(nm_id) if nm_id else None
            chunks.append(self._create_order_chunk(order, product_name))
        
        for product in data.get('products', []):
            chunks.append(self._create_product_chunk(product))
        
        for stock in data.get('stocks', []):
            nm_id = stock.get('nm_id')
            product_name = product_names.get(nm_id) if nm_id else None
            chunks.append(self._create_stock_chunk(stock, product_name))
        
        for review in data.get('reviews', []):
            nm_id = review.get('nm_id')
            product_name = product_names.get(nm_id) if nm_id else None
            chunks.append(self._create_review_chunk(review, product_name))
        
        for sale in data.get('sales', []):
            nm_id = sale.get('nm_id')
            product_name = product_names.get(nm_id) if nm_id else None
            chunks.append(self._create_sale_chunk(sale, product_name))
        
        logger.info(f"üìù Created {len(chunks)} chunks from data")
        
        return chunks
    
    async def generate_embeddings(self, chunks: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of text chunks using CometAPI.
        Uses batch processing and includes retry logic for each batch.
        """
        import time

        if not chunks:
            return []

        all_embeddings = []
        total_chunks = len(chunks)
        failed_batches = []

        for i in range(0, total_chunks, self.batch_size):
            batch = chunks[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            total_batches = (total_chunks + self.batch_size - 1) // self.batch_size

            logger.info(
                f"üîÑ Generating embeddings via CometAPI: batch {batch_num}/{total_batches} "
                f"({len(batch)} chunks)"
            )

            max_retries = 5
            retry_delay = 2

            for attempt in range(max_retries):
                try:
                    response = await comet_client.create_embeddings(
                        texts=batch
                    )
                    
                    batch_embeddings = [item['embedding'] for item in response['data']]
                    all_embeddings.extend(batch_embeddings)

                    usage = response.get("usage")
                    if usage:
                        prompt_tokens = usage.get("prompt_tokens")
                        total_tokens = usage.get("total_tokens")
                        logger.info(
                            f"üßÆ Embedding tokens (batch {batch_num}): "
                            f"prompt={prompt_tokens}, total={total_tokens}"
                        )

                    logger.info(
                        f"‚úÖ Batch {batch_num} processed: {len(batch_embeddings)} embeddings"
                    )
                    break

                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)
                        logger.warning(
                            f"‚ö†Ô∏è Batch {batch_num} failed (attempt {attempt + 1}/{max_retries}): {e}. "
                            f"Retrying in {wait_time}s..."
                        )
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(
                            f"‚ùå Batch {batch_num} failed after {max_retries} attempts: {e}. "
                            f"Skipping batch and continuing..."
                        )
                        failed_batches.append({
                            'batch_num': batch_num,
                            'start_idx': i,
                            'end_idx': i + len(batch),
                            'error': str(e)
                        })
                        break

        if failed_batches:
            logger.warning(
                f"‚ö†Ô∏è Indexing completed with errors: {len(failed_batches)} batches failed. "
                f"Successfully processed: {len(all_embeddings)}/{total_chunks} chunks"
            )
            logger.warning(f"Failed batches info: {failed_batches}")
        else:
            logger.info(f"‚úÖ Generated {len(all_embeddings)} embeddings total")

        return all_embeddings
    
    def save_to_vector_db(
        self,
        embeddings: List[List[float]],
        chunks_metadata: List[Dict[str, Any]],
        cabinet_id: int,
        db: Session
    ) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î.

        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã: –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏.
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–∏—á–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ —á–∞–Ω–∫–∏.

        –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç batch commits (–∫–∞–∂–¥—ã–µ 100 —á–∞–Ω–∫–æ–≤) –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è
        –¥–ª–∏–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±–ª–æ–∫–∏—Ä—É—é—Ç –¥–æ—Å—Ç—É–ø –∫ RAG search.

        Args:
            embeddings: –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
            chunks_metadata: –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            cabinet_id: ID –∫–∞–±–∏–Ω–µ—Ç–∞
            db: –°–µ—Å—Å–∏—è –ë–î

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        if len(embeddings) != len(chunks_metadata):
            logger.warning(
                f"‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω: —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({len(embeddings)}) != "
                f"–º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ({len(chunks_metadata)}). "
                f"–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏."
            )
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —á–∞–Ω–∫–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            min_length = min(len(embeddings), len(chunks_metadata))
            embeddings = embeddings[:min_length]
            chunks_metadata = chunks_metadata[:min_length]
            logger.info(f"üìä –ë—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {min_length} —á–∞–Ω–∫–æ–≤")

        saved_count = 0
        batch_size = 100  # –ö–æ–º–º–∏—Ç–∏–º –∫–∞–∂–¥—ã–µ 100 —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π

        try:
            for idx, (embedding, chunk_meta) in enumerate(zip(embeddings, chunks_metadata)):
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∑–∞–ø–∏—Å—å
                existing_metadata = db.query(RAGMetadata).filter(
                    RAGMetadata.cabinet_id == cabinet_id,
                    RAGMetadata.source_table == chunk_meta['source_table'],
                    RAGMetadata.source_id == chunk_meta['source_id']
                ).first()

                if existing_metadata:
                    # –û–±–Ω–æ–≤–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                    existing_metadata.chunk_text = chunk_meta['chunk_text']
                    existing_metadata.chunk_type = chunk_meta['chunk_type']
                    existing_metadata.chunk_hash = self.calculate_chunk_hash(chunk_meta['chunk_text'])
                    from datetime import timezone
                    existing_metadata.updated_at = datetime.now(timezone.utc)

                    # –û–±–Ω–æ–≤–∏—Ç—å embedding
                    existing_embedding = db.query(RAGEmbedding).filter(
                        RAGEmbedding.metadata_id == existing_metadata.id
                    ).first()

                    if existing_embedding:
                        existing_embedding.embedding = embedding
                        from datetime import timezone
                        existing_embedding.updated_at = datetime.now(timezone.utc)
                    else:
                        # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π embedding (–µ—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ –µ–≥–æ –Ω–µ—Ç)
                        new_embedding = RAGEmbedding(
                            embedding=embedding,
                            metadata_id=existing_metadata.id
                        )
                        db.add(new_embedding)

                else:
                    # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                    new_metadata = RAGMetadata(
                        cabinet_id=cabinet_id,
                        source_table=chunk_meta['source_table'],
                        source_id=chunk_meta['source_id'],
                        chunk_type=chunk_meta['chunk_type'],
                        chunk_text=chunk_meta['chunk_text'],
                        chunk_hash=self.calculate_chunk_hash(chunk_meta['chunk_text'])
                    )
                    db.add(new_metadata)
                    db.flush()  # –ü–æ–ª—É—á–∏—Ç—å ID –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏

                    new_embedding = RAGEmbedding(
                        embedding=embedding,
                        metadata_id=new_metadata.id
                    )
                    db.add(new_embedding)

                saved_count += 1

                # Batch commit: –∫–æ–º–º–∏—Ç–∏–º –∫–∞–∂–¥—ã–µ batch_size —á–∞–Ω–∫–æ–≤
                # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –±–ª–æ–∫–∏—Ä—É—é—Ç RAG search
                if (idx + 1) % batch_size == 0:
                    db.commit()
                    logger.debug(f"üì¶ Batch committed: {saved_count}/{len(embeddings)} chunks saved")

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–º–∏—Ç –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π
            db.commit()
            logger.info(f"‚úÖ Saved {saved_count} records to vector DB (batch size: {batch_size})")

        except Exception as e:
            db.rollback()
            logger.error(f"‚ùå Error saving to vector DB: {e}")
            raise

        return saved_count
    
    async def index_cabinet(
        self,
        cabinet_id: int,
        full_rebuild: bool = False,
        changed_ids: Optional[Dict[str, List[int]]] = None
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–∞–±–∏–Ω–µ—Ç–∞.

        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞:
        1. Event-driven (changed_ids): –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        2. Full rebuild: –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å –æ—á–∏—Å—Ç–∫–æ–π —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö

        Args:
            cabinet_id: ID –∫–∞–±–∏–Ω–µ—Ç–∞
            full_rebuild: –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (weekly cleanup)
            changed_ids: –î–µ–ª—å—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –æ—Ç WB sync (Event-driven)
                {
                    "orders": [12345, 12346],
                    "products": [98765],
                    "stocks": [11111],
                    "reviews": [55555],
                    "sales": [77777]
                }

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            {
                'success': True/False,
                'cabinet_id': int,
                'indexing_mode': 'incremental' | 'full_rebuild',
                'total_chunks': int,
                'metrics': {...},
                'errors': List[str]
            }
        """
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–∂–∏–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        indexing_mode = 'full_rebuild' if full_rebuild else 'incremental'

        result = {
            'success': False,
            'cabinet_id': cabinet_id,
            'indexing_mode': indexing_mode,
            'total_chunks': 0,
            'metrics': {
                'new_chunks': 0,
                'updated_chunks': 0,
                'skipped_chunks': 0,
                'deleted_chunks': 0,
                'embeddings_generated': 0
            },
            'errors': []
        }

        # –ü–æ–ª—É—á–∏—Ç—å —Å–µ—Å—Å–∏—é –ë–î
        db = RAGSessionLocal()
        index_status = None

        try:
            # 1. –ê—Ç–æ–º–∞—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å SELECT FOR UPDATE
            # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç race condition –º–µ–∂–¥—É –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏
            from sqlalchemy import text
            from datetime import timedelta, timezone

            # –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π
            index_status = db.query(RAGIndexStatus).filter(
                RAGIndexStatus.cabinet_id == cabinet_id
            ).with_for_update(nowait=False).first()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            if index_status:
                current_status = index_status.indexing_status

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
                if current_status == 'in_progress':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≤–∏—Å–ª–∞ –ª–∏ –∑–∞–¥–∞—á–∞ (timeout 30 –º–∏–Ω—É—Ç)
                    if index_status.updated_at:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC aware datetime –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                        now_utc = datetime.now(timezone.utc)
                        updated_at = index_status.updated_at

                        # –ï—Å–ª–∏ updated_at naive, –¥–µ–ª–∞–µ–º –µ–≥–æ aware (UTC)
                        if updated_at.tzinfo is None:
                            updated_at = updated_at.replace(tzinfo=timezone.utc)

                        time_since_update = now_utc - updated_at

                        if time_since_update > timedelta(minutes=30):
                            logger.warning(
                                f"‚è∞ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–∞–±–∏–Ω–µ—Ç–∞ {cabinet_id} –∑–∞–≤–∏—Å–ª–∞ "
                                f"(–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ {time_since_update.total_seconds():.0f} —Å–µ–∫—É–Ω–¥ –Ω–∞–∑–∞–¥). "
                                f"–°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º."
                            )
                            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∑–∞–≤–∏—Å—à–∏–π —Å—Ç–∞—Ç—É—Å
                            index_status.indexing_status = 'failed'
                            db.commit()
                        else:
                            logger.warning(
                                f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–∞–±–∏–Ω–µ—Ç–∞ {cabinet_id} —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è "
                                f"(–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ {time_since_update.total_seconds():.0f} —Å–µ–∫—É–Ω–¥ –Ω–∞–∑–∞–¥). –ü—Ä–æ–ø—É—Å–∫."
                            )
                            result['errors'].append("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è")
                            db.rollback()  # –û—Ç–º–µ–Ω—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
                            return result

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ü—Ä–µ–¥—ã–¥—É—â–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å - —Ä–∞–∑—Ä–µ—à–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫
                if current_status == 'failed':
                    logger.info(
                        f"‚ö†Ô∏è –ü—Ä–µ–¥—ã–¥—É—â–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–∞–±–∏–Ω–µ—Ç–∞ {cabinet_id} –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å. "
                        f"–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é."
                    )

            # 2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å 'in_progress' (–∞—Ç–æ–º–∞—Ä–Ω–æ)
            if not index_status:
                index_status = RAGIndexStatus(
                    cabinet_id=cabinet_id,
                    indexing_status='in_progress'
                )
                db.add(index_status)
            else:
                index_status.indexing_status = 'in_progress'
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC aware datetime
                index_status.updated_at = datetime.now(timezone.utc)

            db.commit()  # Commit –∏ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏

            logger.info(
                f"Starting {indexing_mode} indexing for cabinet {cabinet_id}"
                + (f" with {sum(len(ids) for ids in changed_ids.values())} changed IDs" if changed_ids else "")
            )

            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            try:
                if changed_ids and not full_rebuild:
                    # Event-driven: –∏–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    data = await self.extract_data_by_ids(cabinet_id, changed_ids)
                else:
                    # Full rebuild: –∏–∑–≤–ª–µ—á—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
                    data = await self.extract_data_from_main_db(cabinet_id)
            except Exception as e:
                logger.error(f"Error extracting data: {e}")
                result['errors'].append(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
                raise
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
            try:
                chunks_metadata = self.create_chunks(data)
            except Exception as e:
                logger.error(f"‚ùå Error creating chunks: {e}")
                result['errors'].append(f"–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤: {str(e)}")
                raise
            
            if not chunks_metadata:
                logger.warning(f"‚ö†Ô∏è No data to index for cabinet {cabinet_id}")
                index_status.indexing_status = 'completed'
                index_status.total_chunks = 0
                from datetime import timezone
                index_status.last_indexed_at = datetime.now(timezone.utc)
                db.commit()
                result['success'] = True
                return result
            
            # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            try:
                chunk_texts = [chunk['chunk_text'] for chunk in chunks_metadata]
                embeddings = await self.generate_embeddings(chunk_texts)
            except Exception as e:
                logger.error(f"‚ùå Error generating embeddings: {e}")
                result['errors'].append(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
                raise
            
            # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            try:
                saved_count = self.save_to_vector_db(
                    embeddings=embeddings,
                    chunks_metadata=chunks_metadata,
                    cabinet_id=cabinet_id,
                    db=db
                )
            except Exception as e:
                logger.error(f"‚ùå Error saving to DB: {e}")
                result['errors'].append(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î: {str(e)}")
                raise
            
            # 7. –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å 'completed'
            from datetime import timezone
            index_status.indexing_status = 'completed'
            index_status.total_chunks = saved_count
            index_status.updated_at = datetime.now(timezone.utc)

            # –û–±–Ω–æ–≤–∏—Ç—å timestamps –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
            if full_rebuild:
                index_status.last_indexed_at = datetime.now(timezone.utc)
                index_status.last_incremental_at = datetime.now(timezone.utc)  # Full rebuild –æ–±–Ω–æ–≤–ª—è–µ—Ç –æ–±–∞
            else:
                index_status.last_incremental_at = datetime.now(timezone.utc)

            db.commit()

            result['success'] = True
            result['total_chunks'] = saved_count
            result['metrics']['embeddings_generated'] = len(embeddings)

            logger.info(
                f"{indexing_mode.capitalize()} indexing completed for cabinet {cabinet_id}: "
                f"{saved_count} chunks indexed"
            )
            
        except Exception as e:
            # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å 'failed'
            if index_status:
                from datetime import timezone
                index_status.indexing_status = 'failed'
                index_status.updated_at = datetime.now(timezone.utc)
                db.commit()

            logger.error(f"‚ùå Error indexing cabinet {cabinet_id}: {e}")
            result['errors'].append(str(e))
            
        finally:
            db.close()
        
        return result
