"""
Vector Search - –º–æ–¥—É–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ pgvector.

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
–Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
"""

import os
import logging
import re
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text
from gpt_integration.comet_client import comet_client

from .database import RAGSessionLocal
from .models import RAGEmbedding, RAGMetadata

logger = logging.getLogger(__name__)


class VectorSearch:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    
    –ü—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞:
    1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    2. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ pgvector
    3. –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
    """
    
    def __init__(
        self,
        similarity_threshold: Optional[float] = None
    ):
        """
        Initializes the searcher.
        The client for creating embeddings is now the centralized CometClient.
        """
        # Similarity threshold –¥–ª—è cosine similarity (1 - cosine distance), –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        self.similarity_threshold = similarity_threshold or float(
            os.getenv("RAG_SIMILARITY_THRESHOLD", "0.3")
        )
    
    async def generate_query_embedding(self, query_text: str) -> List[float]:
        """
        Generate an embedding for a user query using CometAPI.
        """
        if not query_text or not query_text.strip():
            raise ValueError("Query text cannot be empty")
        
        try:
            logger.info(f"üîÑ Generating embedding via CometAPI for query: '{query_text[:50]}...'")
            
            response = await comet_client.create_embeddings(
                texts=[query_text]
            )
            
            embedding = response['data'][0]['embedding']
            
            usage = response.get("usage")
            if usage:
                prompt_tokens = usage.get("prompt_tokens")
                total_tokens = usage.get("total_tokens")
                logger.info(
                    f"üßÆ Embedding tokens: prompt={prompt_tokens}, total={total_tokens}"
                )
            
            logger.info(f"‚úÖ Embedding generated: dimension {len(embedding)}")
            
            return embedding
            
        except Exception as e:
            logger.error(f"‚ùå Error generating embedding: {e}", exc_info=True)
            raise
    
    def search(
        self,
        query_embedding: List[float],
        cabinet_id: int,
        chunk_types: Optional[List[str]] = None,
        limit: int = 5,
        source_id: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ pgvector.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç cosine distance –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.
        
        Args:
            query_embedding: –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ (—Å–ø–∏—Å–æ–∫ float)
            cabinet_id: ID –∫–∞–±–∏–Ω–µ—Ç–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
            chunk_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            source_id: –§–∏–ª—å—Ç—Ä –ø–æ source_id (–Ω–∞–ø—Ä–∏–º–µ—Ä, nm_id –∏–∑ –∑–∞–ø—Ä–æ—Å–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            [
                {
                    'embedding_id': 1,
                    'metadata_id': 1,
                    'similarity': 0.85,
                    'distance': 0.15
                },
                ...
            ]
        """
        db = RAGSessionLocal()
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è PostgreSQL vector
            embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
            
            # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å SQL –∑–∞–ø—Ä–æ—Å
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º inner product (<=>) –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (OpenAI embeddings)
            # –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: cosine similarity ‚âà inner product
            # pgvector –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç NEGATIVE inner product —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º <=>
            # –ó–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1], –≥–¥–µ -1 = –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ, 1 = –Ω–∞–∏–º–µ–Ω–µ–µ –ø–æ—Ö–æ–∂–∏–µ
            # –ü–æ—ç—Ç–æ–º—É —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ -1 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è similarity –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ASC (–æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É = –æ—Ç —Å–∞–º–æ–≥–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–º—É)
            # –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º f-string –¥–ª—è embedding_str, —Ç–∞–∫ –∫–∞–∫ SQLAlchemy –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            # –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å placeholder —Å ::vector –∏–∑-–∑–∞ –¥–≤–æ–π–Ω–æ–≥–æ –¥–≤–æ–µ—Ç–æ—á–∏—è
            filter_by_chunk_types = bool(chunk_types and len(chunk_types) > 0)
            if filter_by_chunk_types:
                query = text(f"""
                    SELECT
                        e.id AS embedding_id,
                        e.metadata_id,
                        (1 - (e.embedding <=> '{embedding_str}'::vector)) AS similarity,
                        e.embedding <=> '{embedding_str}'::vector AS distance
                    FROM rag_embeddings e
                    JOIN rag_metadata m ON e.metadata_id = m.id
                    WHERE m.cabinet_id = :cabinet_id
                      AND m.chunk_type = ANY(:chunk_types)
                      {"AND m.source_id = :source_id" if source_id is not None else ""}
                    ORDER BY e.embedding <=> '{embedding_str}'::vector ASC
                    LIMIT :limit
                """)
            else:
                query = text(f"""
                    SELECT
                        e.id AS embedding_id,
                        e.metadata_id,
                        (1 - (e.embedding <=> '{embedding_str}'::vector)) AS similarity,
                        e.embedding <=> '{embedding_str}'::vector AS distance
                    FROM rag_embeddings e
                    JOIN rag_metadata m ON e.metadata_id = m.id
                    WHERE m.cabinet_id = :cabinet_id
                      {"AND m.source_id = :source_id" if source_id is not None else ""}
                    ORDER BY e.embedding <=> '{embedding_str}'::vector ASC
                    LIMIT :limit
                """)

            params = {
                'cabinet_id': cabinet_id,
                'limit': limit
            }
            if filter_by_chunk_types:
                params['chunk_types'] = chunk_types
            if source_id is not None:
                params['source_id'] = source_id
            
            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å
            logger.info(
                f"üìä Executing vector search: "
                f"cabinet_id={cabinet_id}, "
                f"limit={limit}, "
                f"chunk_types={chunk_types if chunk_types else 'all'}, "
                f"similarity_threshold={self.similarity_threshold}, "
                f"source_id_filter={source_id if source_id is not None else 'none'}"
            )
            
            result = db.execute(query, params)
            rows = result.fetchall()
            
            logger.info(f"üìà Retrieved {len(rows)} results from DB (before threshold filtering)")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            filtered_out = []
            for idx, row in enumerate(rows):
                similarity = float(row.similarity)
                distance = float(row.distance)

                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                logger.info(
                    f"  [{idx+1}/{len(rows)}] embedding_id={row.embedding_id}, "
                    f"metadata_id={row.metadata_id}, "
                    f"similarity={similarity:.4f} (1 - distance), "
                    f"distance={distance:.4f}"
                )

                # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if similarity >= self.similarity_threshold:
                    results.append({
                        'embedding_id': row.embedding_id,
                        'metadata_id': row.metadata_id,
                        'similarity': similarity,
                        'distance': distance
                    })
                    logger.info(f"    ‚úÖ PASSED threshold ({similarity:.4f} >= {self.similarity_threshold})")
                else:
                    filtered_out.append({
                        'embedding_id': row.embedding_id,
                        'similarity': similarity,
                        'distance': distance
                    })
                    logger.info(f"    ‚ùå FILTERED ({similarity:.4f} < {self.similarity_threshold})")
            
            logger.info(
                f"‚úÖ Filtering results: "
                f"passed={len(results)}, "
                f"filtered_out={len(filtered_out)}, "
                f"threshold={self.similarity_threshold}"
            )
            
            if results:
                logger.info(
                    f"üìä Similarity range for passed results: "
                    f"min={min(r['similarity'] for r in results):.4f}, "
                    f"max={max(r['similarity'] for r in results):.4f}, "
                    f"avg={sum(r['similarity'] for r in results) / len(results):.4f}"
                )
            
            if filtered_out:
                logger.info(
                    f"‚ö†Ô∏è Filtered out results (below threshold {self.similarity_threshold}): "
                    f"min={min(f['similarity'] for f in filtered_out):.4f}, "
                    f"max={max(f['similarity'] for f in filtered_out):.4f}"
                )
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Error in vector search: {e}")
            raise
            
        finally:
            db.close()
    
    def get_metadata_for_embeddings(
        self,
        embedding_ids: List[int],
        db: Session
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö embedding IDs.
        
        Args:
            embedding_ids: –°–ø–∏—Å–æ–∫ ID —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            db: –°–µ—Å—Å–∏—è –ë–î
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:
            [
                {
                    'id': 1,
                    'embedding_id': 1,
                    'chunk_text': '...',
                    'chunk_type': 'order',
                    'source_table': 'wb_orders',
                    'source_id': 123
                },
                ...
            ]
        """
        if not embedding_ids:
            return []
        
        try:
            # –ó–∞–ø—Ä–æ—Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ JOIN
            results = db.query(
                RAGMetadata.id,
                RAGMetadata.chunk_text,
                RAGMetadata.chunk_type,
                RAGMetadata.source_table,
                RAGMetadata.source_id,
                RAGEmbedding.id.label('embedding_id')
            ).join(
                RAGEmbedding,
                RAGMetadata.id == RAGEmbedding.metadata_id
            ).filter(
                RAGEmbedding.id.in_(embedding_ids)
            ).all()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            metadata_list = []
            for row in results:
                metadata_list.append({
                    'id': row.id,
                    'embedding_id': row.embedding_id,
                    'chunk_text': row.chunk_text,
                    'chunk_type': row.chunk_type,
                    'source_table': row.source_table,
                    'source_id': row.source_id
                })
            
            logger.info(f"üìã Retrieved {len(metadata_list)} metadata records")
            
            return metadata_list
            
        except Exception as e:
            logger.error(f"‚ùå Error retrieving metadata: {e}")
            raise
    
    async def search_relevant_chunks(
        self,
        query_text: str,
        cabinet_id: int,
        chunk_types: Optional[List[str]] = None,
        max_chunks: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Main method for finding relevant chunks.
        Combines all steps: query embedding, vector search, and metadata retrieval.
        """
        try:
            # Detect nm_id in the query text to filter more accurately
            nm_id_match = re.search(r"\b\d{6,}\b", query_text)
            nm_id_filter = int(nm_id_match.group()) if nm_id_match else None
            effective_chunk_types = chunk_types
            if nm_id_filter and not effective_chunk_types:
                effective_chunk_types = ["stock", "product"]

            logger.info(
                f"üîç Starting search for relevant chunks:\n"
                f"  üìù Query: '{query_text}'\n"
                f"  üè¢ Cabinet ID: {cabinet_id}\n"
                f"  üì¶ Chunk types: {effective_chunk_types if effective_chunk_types else 'all'}\n"
                f"  üî¢ Max results: {max_chunks}\n"
                f"  üìä Similarity threshold: {self.similarity_threshold}\n"
                f"  üîé Source ID filter: {nm_id_filter if nm_id_filter else 'none'}"
            )
            
            # 1. Generate query embedding
            query_embedding = await self.generate_query_embedding(query_text)
            
            # 2. Vector Search
            search_results = self.search(
                query_embedding=query_embedding,
                cabinet_id=cabinet_id,
                chunk_types=effective_chunk_types,
                limit=max_chunks,
                source_id=nm_id_filter
            )
            
            if not search_results:
                logger.warning(
                    f"‚ö†Ô∏è No relevant chunks found for query '{query_text[:100]}...' "
                    f"(cabinet_id={cabinet_id}, threshold={self.similarity_threshold})"
                )
                return []
            
            logger.info(
                f"üìã Found {len(search_results)} results after filtering, "
                f"fetching metadata..."
            )
            
            # 3. Retrieve metadata
            db = RAGSessionLocal()
            try:
                embedding_ids = [r['embedding_id'] for r in search_results]
                logger.debug(f"üîç Requesting metadata for {len(embedding_ids)} embedding IDs: {embedding_ids}")
                
                metadata_list = self.get_metadata_for_embeddings(embedding_ids, db)
                
                # Combine with similarity scores
                similarity_map = {r['embedding_id']: r['similarity'] for r in search_results}
                for metadata in metadata_list:
                    embedding_id = metadata['embedding_id']
                    metadata['similarity'] = similarity_map.get(embedding_id, 0.0)
                
                # Sort by similarity (descending)
                metadata_list.sort(key=lambda x: x['similarity'], reverse=True)
                
                logger.info(
                    f"‚úÖ Final search results:\n"
                    f"  üìä Total found: {len(metadata_list)} chunks\n"
                    f"  üìà Similarity range: {metadata_list[0]['similarity']:.4f} - "
                    f"{metadata_list[-1]['similarity']:.4f}\n"
                    f"  üìù Chunk types: {', '.join(set(m['chunk_type'] for m in metadata_list))}"
                )
                
                # Detailed log for each result
                for idx, metadata in enumerate(metadata_list, 1):
                    logger.debug(
                        f"  [{idx}] similarity={metadata['similarity']:.4f}, "
                        f"type={metadata['chunk_type']}, "
                        f"source={metadata['source_table']}:{metadata['source_id']}, "
                        f"text_preview='{metadata['chunk_text'][:50]}...'"
                    )
                
                return metadata_list
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"‚ùå Error searching relevant chunks: {e}", exc_info=True)
            # Fallback to empty list on error
            return []
