"""
Vector Search - –º–æ–¥—É–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ pgvector.

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
–Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
"""

import os
import logging
import re
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text
from gpt_integration.comet_client import comet_client

from .database import RAGSessionLocal
from .models import RAGEmbedding, RAGMetadata

logger = logging.getLogger(__name__)


class VectorSearch:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    
    –ü—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞:
    1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    2. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ pgvector
    3. –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
    """
    
    def __init__(
        self,
        similarity_threshold: Optional[float] = None
    ):
        """
        Initializes the searcher.
        The client for creating embeddings is now the centralized CometClient.
        """
        # Similarity threshold –¥–ª—è cosine similarity (1 - cosine distance), –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        self.similarity_threshold = similarity_threshold or float(
            os.getenv("RAG_SIMILARITY_THRESHOLD", "0.3")
        )
    
    async def generate_query_embedding(self, query_text: str) -> List[float]:
        """
        Generate an embedding for a user query using CometAPI.
        """
        if not query_text or not query_text.strip():
            raise ValueError("Query text cannot be empty")
        
        try:
            logger.info(f"üîÑ Generating embedding via CometAPI for query: '{query_text[:50]}...'")
            
            response = await comet_client.create_embeddings(
                texts=[query_text]
            )
            
            embedding = response['data'][0]['embedding']
            
            usage = response.get("usage")
            if usage:
                prompt_tokens = usage.get("prompt_tokens")
                total_tokens = usage.get("total_tokens")
                logger.info(
                    f"üßÆ Embedding tokens: prompt={prompt_tokens}, total={total_tokens}"
                )
            
            logger.info(f"‚úÖ Embedding generated: dimension {len(embedding)}")
            
            return embedding
            
        except Exception as e:
            logger.error(f"‚ùå Error generating embedding: {e}", exc_info=True)
            raise
    
    def search(
        self,
        query_embedding: List[float],
        cabinet_id: int,
        chunk_types: Optional[List[str]] = None,
        limit: int = 5,
        source_id: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ pgvector.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç cosine distance –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.
        
        Args:
            query_embedding: –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ (—Å–ø–∏—Å–æ–∫ float)
            cabinet_id: ID –∫–∞–±–∏–Ω–µ—Ç–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
            chunk_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            source_id: –§–∏–ª—å—Ç—Ä –ø–æ source_id (–Ω–∞–ø—Ä–∏–º–µ—Ä, nm_id –∏–∑ –∑–∞–ø—Ä–æ—Å–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            [
                {
                    'embedding_id': 1,
                    'metadata_id': 1,
                    'similarity': 0.85,
                    'distance': 0.15
                },
                ...
            ]
        """
        db = RAGSessionLocal()
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è PostgreSQL vector
            embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
            
            # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å SQL –∑–∞–ø—Ä–æ—Å
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º inner product (<=>) –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (OpenAI embeddings)
            # –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: cosine similarity ‚âà inner product
            # pgvector –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç NEGATIVE inner product —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º <=>
            # –ó–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1], –≥–¥–µ -1 = –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ, 1 = –Ω–∞–∏–º–µ–Ω–µ–µ –ø–æ—Ö–æ–∂–∏–µ
            # –ü–æ—ç—Ç–æ–º—É —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ -1 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è similarity –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ASC (–æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É = –æ—Ç —Å–∞–º–æ–≥–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–º—É)
            # –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º f-string –¥–ª—è embedding_str, —Ç–∞–∫ –∫–∞–∫ SQLAlchemy –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            # –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å placeholder —Å ::vector –∏–∑-–∑–∞ –¥–≤–æ–π–Ω–æ–≥–æ –¥–≤–æ–µ—Ç–æ—á–∏—è
            filter_by_chunk_types = bool(chunk_types and len(chunk_types) > 0)
            if filter_by_chunk_types:
                query = text(f"""
                    SELECT
                        e.id AS embedding_id,
                        e.metadata_id,
                        (1 - (e.embedding <=> '{embedding_str}'::vector)) AS similarity,
                        e.embedding <=> '{embedding_str}'::vector AS distance,
                        m.source_id
                    FROM rag_embeddings e
                    JOIN rag_metadata m ON e.metadata_id = m.id
                    WHERE m.cabinet_id = :cabinet_id
                      AND m.chunk_type = ANY(:chunk_types)
                      {"AND m.source_id = :source_id" if source_id is not None else ""}
                    ORDER BY e.embedding <=> '{embedding_str}'::vector ASC, m.source_id DESC
                    LIMIT :limit
                """)
            else:
                query = text(f"""
                    SELECT
                        e.id AS embedding_id,
                        e.metadata_id,
                        (1 - (e.embedding <=> '{embedding_str}'::vector)) AS similarity,
                        e.embedding <=> '{embedding_str}'::vector AS distance,
                        m.source_id
                    FROM rag_embeddings e
                    JOIN rag_metadata m ON e.metadata_id = m.id
                    WHERE m.cabinet_id = :cabinet_id
                      {"AND m.source_id = :source_id" if source_id is not None else ""}
                    ORDER BY e.embedding <=> '{embedding_str}'::vector ASC, m.source_id DESC
                    LIMIT :limit
                """)

            params = {
                'cabinet_id': cabinet_id,
                'limit': limit
            }
            if filter_by_chunk_types:
                params['chunk_types'] = chunk_types
            if source_id is not None:
                params['source_id'] = source_id
            
            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å
            logger.info(
                f"üìä Executing vector search: "
                f"cabinet_id={cabinet_id}, "
                f"limit={limit}, "
                f"chunk_types={chunk_types if chunk_types else 'all'}, "
                f"similarity_threshold={self.similarity_threshold}, "
                f"source_id_filter={source_id if source_id is not None else 'none'}"
            )
            
            result = db.execute(query, params)
            rows = result.fetchall()
            
            logger.info(f"üìà Retrieved {len(rows)} results from DB (before threshold filtering)")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            filtered_out = []
            for idx, row in enumerate(rows):
                similarity = float(row.similarity)
                distance = float(row.distance)

                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                logger.info(
                    f"  [{idx+1}/{len(rows)}] embedding_id={row.embedding_id}, "
                    f"metadata_id={row.metadata_id}, "
                    f"similarity={similarity:.4f} (1 - distance), "
                    f"distance={distance:.4f}"
                )

                # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if similarity >= self.similarity_threshold:
                    results.append({
                        'embedding_id': row.embedding_id,
                        'metadata_id': row.metadata_id,
                        'similarity': similarity,
                        'distance': distance,
                        'source_id': row.source_id  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è debug –∏ future use
                    })
                    logger.info(f"    ‚úÖ PASSED threshold ({similarity:.4f} >= {self.similarity_threshold})")
                else:
                    filtered_out.append({
                        'embedding_id': row.embedding_id,
                        'similarity': similarity,
                        'distance': distance
                    })
                    logger.info(f"    ‚ùå FILTERED ({similarity:.4f} < {self.similarity_threshold})")
            
            logger.info(
                f"‚úÖ Filtering results: "
                f"passed={len(results)}, "
                f"filtered_out={len(filtered_out)}, "
                f"threshold={self.similarity_threshold}"
            )
            
            if results:
                logger.info(
                    f"üìä Similarity range for passed results: "
                    f"min={min(r['similarity'] for r in results):.4f}, "
                    f"max={max(r['similarity'] for r in results):.4f}, "
                    f"avg={sum(r['similarity'] for r in results) / len(results):.4f}"
                )
            
            if filtered_out:
                logger.info(
                    f"‚ö†Ô∏è Filtered out results (below threshold {self.similarity_threshold}): "
                    f"min={min(f['similarity'] for f in filtered_out):.4f}, "
                    f"max={max(f['similarity'] for f in filtered_out):.4f}"
                )
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Error in vector search: {e}")
            raise
            
        finally:
            db.close()
    
    def get_metadata_for_embeddings(
        self,
        embedding_ids: List[int],
        db: Session
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö embedding IDs.
        
        Args:
            embedding_ids: –°–ø–∏—Å–æ–∫ ID —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            db: –°–µ—Å—Å–∏—è –ë–î
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:
            [
                {
                    'id': 1,
                    'embedding_id': 1,
                    'chunk_text': '...',
                    'chunk_type': 'order',
                    'source_table': 'wb_orders',
                    'source_id': 123
                },
                ...
            ]
        """
        if not embedding_ids:
            return []
        
        try:
            # –ó–∞–ø—Ä–æ—Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ JOIN
            results = db.query(
                RAGMetadata.id,
                RAGMetadata.chunk_text,
                RAGMetadata.chunk_type,
                RAGMetadata.source_table,
                RAGMetadata.source_id,
                RAGEmbedding.id.label('embedding_id')
            ).join(
                RAGEmbedding,
                RAGMetadata.id == RAGEmbedding.metadata_id
            ).filter(
                RAGEmbedding.id.in_(embedding_ids)
            ).all()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            metadata_list = []
            for row in results:
                metadata_list.append({
                    'id': row.id,
                    'embedding_id': row.embedding_id,
                    'chunk_text': row.chunk_text,
                    'chunk_type': row.chunk_type,
                    'source_table': row.source_table,
                    'source_id': row.source_id
                })
            
            logger.info(f"üìã Retrieved {len(metadata_list)} metadata records")
            
            return metadata_list
            
        except Exception as e:
            logger.error(f"‚ùå Error retrieving metadata: {e}")
            raise
    
    def _detect_chunk_types_from_query(self, query_text: str) -> Optional[List[str]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø—ã —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ.

        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç chunk_types –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö.
        –≠—Ç–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–æ–∑–≤—Ä–∞—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤ ['sale', 'order', 'product', 'stock', 'review'] –∏–ª–∏ None (–≤—Å–µ —Ç–∏–ø—ã)
        """
        query_lower = query_text.lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —á–∞–Ω–∫–æ–≤
        keywords_map = {
            'sale': [
                '–≤—ã–∫—É–ø', '–≤—ã–∫—É–ø–æ–≤', '–≤—ã–∫—É–ø—ã', '–≤—ã–∫—É–ø–∞',
                '–ø—Ä–æ–¥–∞–∂', '–ø—Ä–æ–¥–∞–∂–∏', '–ø—Ä–æ–¥–∞–∂–∞', '–ø—Ä–æ–¥–∞–∂–µ',
                '–≤–æ–∑–≤—Ä–∞—Ç', '–≤–æ–∑–≤—Ä–∞—Ç–æ–≤', '–≤–æ–∑–≤—Ä–∞—Ç—ã', '–≤–æ–∑–≤—Ä–∞—Ç–∞',
                'sales', 'sale', 'buyout', 'return',
                '–∫—É–ø–∏', '–∫—É–ø–∏–ª–∏', '–∫—É–ø–∏–ª',  # "–∫—Ç–æ –∫—É–ø–∏–ª —Ç–æ–≤–∞—Ä"
                '–ø—Ä–æ–¥–∞–Ω–Ω',  # –ø—Ä–æ–¥–∞–Ω–Ω—ã–π, –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–¥–∞–Ω–Ω—ã–µ
            ],
            'order': [
                '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑–æ–≤', '–∑–∞–∫–∞–∑—ã', '–∑–∞–∫–∞–∑–∞',
                'order', 'orders',
                '–æ—Ç–º–µ–Ω',  # –æ—Ç–º–µ–Ω–µ–Ω, –æ—Ç–º–µ–Ω–µ–Ω–Ω—ã—Ö, –æ—Ç–º–µ–Ω—ã
                '–Ω–æ–≤—ã—Ö –∑–∞–∫–∞–∑', '–ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–∫–∞–∑',
            ],
            'product': [
                '—Ç–æ–≤–∞—Ä', '—Ç–æ–≤–∞—Ä–æ–≤', '—Ç–æ–≤–∞—Ä—ã', '—Ç–æ–≤–∞—Ä–∞',
                'product', 'products',
                '–∞—Ä—Ç–∏–∫—É–ª', 'nm_id', 'nmid',
                '—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '—Ü–µ–Ω–æ–π',
                '—Ä–µ–π—Ç–∏–Ω–≥', '—Ä–µ–π—Ç–∏–Ω–≥–∞',
                '–±—Ä–µ–Ω–¥', '–±—Ä–µ–Ω–¥–∞',
                '–∫–∞—Ç–µ–≥–æ—Ä–∏',  # –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            ],
            'stock': [
                '–æ—Å—Ç–∞—Ç', '–æ—Å—Ç–∞—Ç–∫–æ–≤', '–æ—Å—Ç–∞—Ç–∫–∏', '–æ—Å—Ç–∞—Ç–∫–∞',
                'stock', 'stocks',
                '—Å–∫–ª–∞–¥', '—Å–∫–ª–∞–¥–æ–≤', '—Å–∫–ª–∞–¥–µ', '—Å–∫–ª–∞–¥–∞—Ö',
                'warehouse',
                '–∫–æ–ª–∏—á–µ—Å—Ç–≤', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–∞',
                '—Å–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–ª–æ—Å—å', '—Å–∫–æ–ª—å–∫–æ –Ω–∞ —Å–∫–ª–∞–¥–µ',
                '–≥–¥–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è', '–≥–¥–µ —Ç–æ–≤–∞—Ä',
            ],
            'review': [
                '–æ—Ç–∑—ã–≤', '–æ—Ç–∑—ã–≤–æ–≤', '–æ—Ç–∑—ã–≤—ã', '–æ—Ç–∑—ã–≤–∞',
                'review', 'reviews',
                '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π',
                '–æ—Ü–µ–Ω–∫', '–æ—Ü–µ–Ω–æ–∫', '–æ—Ü–µ–Ω–∫–∞',
                '–Ω–µ–≥–∞—Ç–∏–≤', '–ø–æ–∑–∏—Ç–∏–≤',
                '–ø–ª–æ—Ö', '—Ö–æ—Ä–æ—à',  # –ø–ª–æ—Ö–∏–µ –æ—Ç–∑—ã–≤—ã, —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–∑—ã–≤—ã
            ]
        }

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        matches = {}
        for chunk_type, keywords in keywords_map.items():
            match_count = sum(1 for keyword in keywords if keyword in query_lower)
            if match_count > 0:
                matches[chunk_type] = match_count

        # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π - –≤–µ—Ä–Ω—É—Ç—å None (–∏—Å–∫–∞—Ç—å –≤–æ –≤—Å–µ—Ö —Ç–∏–ø–∞—Ö)
        if not matches:
            return None

        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–π winner (–Ω–∞ 2+ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –±–æ–ª—å—à–µ) - –≤–µ—Ä–Ω—É—Ç—å —Ç–æ–ª—å–∫–æ –µ–≥–æ
        max_matches = max(matches.values())
        top_types = [t for t, count in matches.items() if count == max_matches]

        # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∏–ø–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º —Å—á–µ—Ç–æ–º - –≤–µ—Ä–Ω—É—Ç—å –≤—Å–µ (–Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å)
        # –ù–æ –µ—Å–ª–∏ –æ–¥–∏–Ω —è–≤–Ω–æ –ª–∏–¥–∏—Ä—É–µ—Ç - –≤–µ—Ä–Ω—É—Ç—å —Ç–æ–ª—å–∫–æ –µ–≥–æ
        if len(top_types) == 1:
            logger.info(f"üéØ Detected chunk_type from query: {top_types[0]} (matches: {max_matches})")
            return top_types
        else:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Ç–∏–ø–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º —Å—á–µ—Ç–æ–º - –≤–µ—Ä–Ω—É—Ç—å –∏—Ö –≤—Å–µ
            logger.info(f"üéØ Detected multiple chunk_types from query: {top_types} (matches: {max_matches})")
            return top_types

    def _is_temporal_query(self, query_text: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º (—Ç—Ä–µ–±—É–µ—Ç —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –¥–∞—Ç–µ).

        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            True –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        """
        temporal_keywords = [
            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–ø–æ—Å–ª–µ–¥–Ω–∏–µ/–Ω–µ–¥–∞–≤–Ω–∏–µ)
            '–ø–æ—Å–ª–µ–¥–Ω',  # –ø–æ—Å–ª–µ–¥–Ω–∏—Ö, –ø–æ—Å–ª–µ–¥–Ω–∏–µ, –ø–æ—Å–ª–µ–¥–Ω–∏–π
            '–Ω–µ–¥–∞–≤–Ω',  # –Ω–µ–¥–∞–≤–Ω–∏—Ö, –Ω–µ–¥–∞–≤–Ω–∏–µ
            '–Ω–æ–≤—ã—Ö', '–Ω–æ–≤—ã–µ', '–Ω–æ–≤—ã–π',
            '—Å–≤–µ–∂–∏—Ö', '—Å–≤–µ–∂–∏–µ', '—Å–≤–µ–∂–∏–π',
            'recent', 'latest',

            # –ê–±—Å–æ–ª—é—Ç–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–≥–æ–¥–Ω—è/–≤—á–µ—Ä–∞)
            '—Å–µ–≥–æ–¥–Ω—è', 'today',
            '–≤—á–µ—Ä–∞', 'yesterday',
            '–ø–æ–∑–∞–≤—á–µ—Ä–∞',

            # –ü–µ—Ä–∏–æ–¥—ã
            '–∑–∞ –¥–µ–Ω—å', '–∑–∞ —Å—É—Ç–∫–∏',
            '–∑–∞ –Ω–µ–¥–µ–ª—é', '–Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ', 'this week',
            '–∑–∞ –º–µ—Å—è—Ü', '–≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ', 'this month',
            '–∑–∞ –≥–æ–¥', '–≤ —ç—Ç–æ–º –≥–æ–¥—É', 'this year',

            # –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
            '—Ç–µ–ø–µ—Ä—å', '—Å–µ–π—á–∞—Å', 'now', 'currently',
        ]
        query_lower = query_text.lower()
        return any(keyword in query_lower for keyword in temporal_keywords)

    def _extract_date_from_chunk(self, chunk_text: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–∞–Ω–∫–∞.

        –ò—â–µ—Ç –¥–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–æ—Ç 2025-12-18").

        Args:
            chunk_text: –¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞

        Returns:
            –î–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD –∏–ª–∏ None
        """
        # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
        date_pattern = r'–æ—Ç (\d{4}-\d{2}-\d{2})'
        match = re.search(date_pattern, chunk_text)
        if match:
            return match.group(1)
        return None

    def _rerank_by_date(self, metadata_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –¥–∞—Ç–µ (–æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º).

        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç semantic similarity —Å temporal ranking:
        - –°–Ω–∞—á–∞–ª–∞ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ –¥–∞—Ç–µ (DESC)
        - –ó–∞—Ç–µ–º –ø–æ similarity (DESC) –¥–ª—è –∑–∞–ø–∏—Å–µ–π —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–∞—Ç–æ–π

        Args:
            metadata_list: –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å similarity scores

        Returns:
            –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã –∏–∑ chunk_text
        for metadata in metadata_list:
            chunk_date = self._extract_date_from_chunk(metadata['chunk_text'])
            metadata['extracted_date'] = chunk_date or '0000-00-00'  # Fallback –¥–ª—è –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –¥–∞—Ç—ã

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –¥–∞—Ç–µ (DESC), –∑–∞—Ç–µ–º –ø–æ similarity (DESC)
        metadata_list.sort(
            key=lambda x: (x['extracted_date'], x['similarity']),
            reverse=True
        )

        logger.info(
            f"üîÑ Re-ranked by date: "
            f"date_range={metadata_list[0]['extracted_date']} to {metadata_list[-1]['extracted_date']}"
        )

        return metadata_list

    async def search_relevant_chunks(
        self,
        query_text: str,
        cabinet_id: int,
        chunk_types: Optional[List[str]] = None,
        max_chunks: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Main method for finding relevant chunks.
        Combines all steps: query embedding, vector search, and metadata retrieval.
        """
        try:
            # Detect nm_id in the query text to filter more accurately
            nm_id_match = re.search(r"\b\d{6,}\b", query_text)
            nm_id_filter = int(nm_id_match.group()) if nm_id_match else None

            # Auto-detect chunk types from query keywords (if not explicitly provided)
            if chunk_types is None:
                detected_types = self._detect_chunk_types_from_query(query_text)
                effective_chunk_types = detected_types
            else:
                # Use explicitly provided chunk_types
                effective_chunk_types = chunk_types
                logger.info(f"üìå Using explicitly provided chunk_types: {chunk_types}")

            # Special case: nm_id detected but no chunk_types specified
            if nm_id_filter and effective_chunk_types is None:
                effective_chunk_types = ["stock", "product"]
                logger.info(f"üîç nm_id detected, limiting to stock/product")

            # Detect temporal queries
            is_temporal = self._is_temporal_query(query_text)

            logger.info(
                f"üîç Starting search for relevant chunks:\n"
                f"  üìù Query: '{query_text}'\n"
                f"  üè¢ Cabinet ID: {cabinet_id}\n"
                f"  üì¶ Chunk types: {effective_chunk_types if effective_chunk_types else 'all'}\n"
                f"  üî¢ Max results: {max_chunks}\n"
                f"  üìä Similarity threshold: {self.similarity_threshold}\n"
                f"  üîé Source ID filter: {nm_id_filter if nm_id_filter else 'none'}\n"
                f"  ‚è∞ Temporal query: {is_temporal}"
            )

            # 1. Generate query embedding
            query_embedding = await self.generate_query_embedding(query_text)

            # 2. Vector Search (fetch more results for temporal queries to have better date coverage)
            search_limit = max_chunks * 3 if is_temporal else max_chunks
            search_results = self.search(
                query_embedding=query_embedding,
                cabinet_id=cabinet_id,
                chunk_types=effective_chunk_types,
                limit=search_limit,
                source_id=nm_id_filter
            )

            if not search_results:
                logger.warning(
                    f"‚ö†Ô∏è No relevant chunks found for query '{query_text[:100]}...' "
                    f"(cabinet_id={cabinet_id}, threshold={self.similarity_threshold})"
                )
                return []

            logger.info(
                f"üìã Found {len(search_results)} results after filtering, "
                f"fetching metadata..."
            )

            # 3. Retrieve metadata
            db = RAGSessionLocal()
            try:
                embedding_ids = [r['embedding_id'] for r in search_results]
                logger.debug(f"üîç Requesting metadata for {len(embedding_ids)} embedding IDs: {embedding_ids}")

                metadata_list = self.get_metadata_for_embeddings(embedding_ids, db)

                # Combine with similarity scores
                similarity_map = {r['embedding_id']: r['similarity'] for r in search_results}
                for metadata in metadata_list:
                    embedding_id = metadata['embedding_id']
                    metadata['similarity'] = similarity_map.get(embedding_id, 0.0)

                # 4. Sort: by date if temporal, otherwise by similarity
                if is_temporal:
                    logger.info("‚è∞ Temporal query detected - re-ranking by date")
                    metadata_list = self._rerank_by_date(metadata_list)
                    # Limit to max_chunks after re-ranking
                    metadata_list = metadata_list[:max_chunks]
                else:
                    # Sort by similarity (descending)
                    metadata_list.sort(key=lambda x: x['similarity'], reverse=True)

                logger.info(
                    f"‚úÖ Final search results:\n"
                    f"  üìä Total found: {len(metadata_list)} chunks\n"
                    f"  üìà Similarity range: {metadata_list[0]['similarity']:.4f} - "
                    f"{metadata_list[-1]['similarity']:.4f}\n"
                    f"  üìù Chunk types: {', '.join(set(m['chunk_type'] for m in metadata_list))}"
                )

                # Detailed log for each result
                for idx, metadata in enumerate(metadata_list, 1):
                    logger.debug(
                        f"  [{idx}] similarity={metadata['similarity']:.4f}, "
                        f"type={metadata['chunk_type']}, "
                        f"source={metadata['source_table']}:{metadata['source_id']}, "
                        f"text_preview='{metadata['chunk_text'][:50]}...'"
                    )

                return metadata_list

            finally:
                db.close()

        except Exception as e:
            logger.error(f"‚ùå Error searching relevant chunks: {e}", exc_info=True)
            # Fallback to empty list on error
            return []
